---
title: "Inside Out: Externalizing Assumptions in Data Analysis as Validation Checks"
abstract: "In data analysis, unexpected results often prompt researchers to revisit their procedures to identify potential issues. While experienced researchers can often quickly diagnose problems by checking a few key assumptions, others may struggle to identify the root causes. These checked assumptions, or expectations, are typically informal, difficult to trace, and rarely discussed in publications. In this paper, we formalize these informal assumptions by framing them as binary *analysis validation checks*. We then introduce a procedure to quantify how violations of these checks may lead to unexpected results in the analysis. The procedure relies on simulations of the original data and evaluates both accuracy and redundancy. Accuracy is calculated through a binary classification metric, while redundancy is measured using mutual information. We demonstrate this approach with a toy example based on fitness step count data and a generalized linear model example examining the effect of particulate matter air pollution on daily mortality. "
author:
  - name: H. Sherry Zhang
    email: huize.zhang@austin.utexas.edu
    affiliations: 
        - id: 1
          name: University of Texas at Austin
          department: Department of Statistics and Data Sciences
          city: Texas
          country: United States
          postal-code: 78705
    attributes:
        corresponding: true
  - name: Roger D. Peng
    email: roger.peng@austin.utexas.edu
    affiliations: 
        - id: 1
          name: University of Texas at Austin
          department: Department of Statistics and Data Sciences
          city: Texas
          country: United States
          postal-code: 78705
cite-method: natbib
bibliography: references.bib
format:
 tandf-pdf:
   keep-tex: true
   fontsize: 12pt
   linestretch: 1.15
   notebook-links: false
preamble: >
  \usepackage{amsfonts,amsmath,amssymb,amsthm}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(patchwork)
library(MASS)
library(ggh4x)
library(adtoolbox)
library(broom)
library(knitr)
library(rpart)
library(infotheo)
library(patchwork)
pm10 <- read_csv(here::here("data", "pm10.csv")) |> 
  filter(!is.na(pm10)) |>    
  mutate(season = as.factor(season))  
```

# Introduction

In data analysis, experienced researchers often rely on their prior knowledge or domain expertise to quickly assess whether results align with their expectations. When a result falls outside of this interval, it prompts the researchers to investigate backwards on the data quality, the analysis steps, or the assumptions made during the analysis process. This mental process of where to diagnose unexpected outcomes is often difficult to trace and discuss in publications. As a result, readers are typically presented with the final outcomes of the analysis cycle where the results and expectations are aligned, achieved either by refining the analysis or updating the expectations based on statistical evidence [@grolemund_cognitive_2014]. These missing pieces of information provides little guidance for diagnosing issues in the analysis when the same methodology is applied to a new dataset that produces different outcomes. Similarly, when researchers with different background knowledge view results that they find to be unexpected, it becomes unclear whether discrepancies arise from differing expectations or from the use of statistical techniques.

One might gain insight into analysts' thought processes by speaking with them directly or watching them work via screencast videos they produce, such as, TidyTuesday screencast videos or think-aloud type studies [e.g. @gu2024data]. However, direct observation of analysis is not scalable and may not always be feasible; creating educational screencast videos requires significant effort from the researchers. Ideally, there could be a way to make expectations about data analysis explicit and accessible to others. Even better, if the encoding were machine-readable, we could analyze these expectations and learn from the analysis itself. For example, we could answer questions about whether the checks also apply to other researchers analyzing new data in the same context, whether they reflect common practices in the field, or whether they are specific to the data or analysis at hand. 

The externalization of the data analysis process is a practice that has potential to improve the trustworthiness of analyses in general and the trustworthiness of subsequent products, such as machine learning models, that may be built on such analyses. While publication of analysis code and data is now a common a requirement for the sake of reproducibility [@peng2011reproducible], the publication code alone is often insufficient for understanding the thought process behind an analysis. Code corresponding to published analyses often reflect the final decisions made about analysis and do not reveal the decisions or assumptions made about the data processing. Thus, a reader looking at published code can often be left with many questions about why certain choices were made. Developing an approach to reveal some of this process without requiring a reader to essentially reconstruct the analysis process from the raw data would provide an improved basis for trusting an analysis result [@peng2021reproducible].

In this paper, we conceptualize these internal expectations and assumptions as *analysis validation checks*, which allows us to examine the assumptions made during an analysis and to diagnose unexpected outcomes. We then introduce a procedure that provides a quantitative measure of how violations in a tree of analysis checks, derived from individual checks, will lead to an unexpected result. The procedure, based on simulations of the original data, calculates the accuracy and redundancy of the analysis checks. Accuracy is determined using binary classification metrics, precision and recall, from a logic regression fit [@ruczinski_logic_2003], while redundancy is measured using mutual information. The proposed workflow offers a numerical guarantee that the analysis will produce the expected results, assuming the assumptions about the data generating mechanism hold.

The rest of the paper is organized as follows: @sec-lit-review reviews the concepts of diagnosing unexpected outcomes and general data quality checks. @sec-plan introduces the concept of analysis validation checks, illustrated with a toy example based on fitness step count data. @sec-method describes the procedure that quantifies how analysis validation checks combined using logical operators can predict unexpected outcomes in an analysis. @sec-pm10-mortality applies this procedure to a larger example that estimates the effect of particulate matter air pollution on daily mortality. @sec-discussion summarises teh paper and discusses a few key considerations.

# Related Work {#sec-lit-review}

## Diagnosing unexpected outcomes in data analysis

The concept of framing data analysis as a sense-making process was originally presented by [@grolemund_cognitive_2014] based on seminal work by [@wild1999statistical]. Key to any sense-making process is a model for the world (i.e. expectations for what we might observe) and observed data with which we can compare our expectations. If there is a significant deviation between what we observe and our expectations, then a data analysis must determine what is causing that deviation. A naive approach would be to update our model for the world to match the data, under the assumption that the initial expectation was incorrect. However, experienced analysts know that the reality can be more nuanced than that, with errors occurring in data collection or data processing that can have an impact on final results. 

The skill of diagnosing unexpected data analysis results is not one that has received significant attention in the statistics literature. While the concept of diagnosis is often embedded in model checking or data visualization techniques, systematic approaches to identifying the root cause of an unexpected analysis result are typically not presented [@peng2022perspective]. [@peng_diagnosing_2021] proposed a series of exercises for training students in data analysis to diagnose different kinds of analysis problems such as coding errors or outliers. They provide a systematic approach involving working backwards from the analysis result to identify potential causes. There are parallels here to the concept of debugging and testing in software engineering [@donoghue2021teaching]. For example, [@li2019towards] found that experienced engineers were generally able to identify problems in code faster than novices, and that the ability to debug code required knowledge that cut across different domains. 

If it is true that the speed with which data analysts can identify problems with an analysis is related to their experience working with a given type of data, then there is perhaps room to improve the analytic process by externalizing the aspects that an analyst learns through experience. That way, inexperienced analysts could examine the thought process of an experienced analyst and learn to identify factors that can cause unexpected results to occur.

## Data analysis checks 

A substantial body of literature has addressed the definition of data quality [@8642813, more] and has developed frameworks that include dimensions, attributes, and measures to evaluate and improve data quality [@cai2015challenges; @wang1996beyond; @6204995; @woodall2014classification]. These frameworks are often used in  information systems and database management and support business decision-making in various industries. For research purposes, high-quality data ensures the credibility of scientific findings and supports reproducibility and re-usability in future studies [ref]. With the growing prevalence of open data in scientific research, the consumers or users of the data typically are no longer the producers or collectors of the data who would have the full knowledge of data in hand, prompting more interest towards data quality checks in the data analysis process. In R, there are some packages, like `skimr` [@skimr] and `dataMaid` [@dataMaid], that provide basic data screening and reporting tools, while another class of packages, e.g. `assertr` [@assertr], `validate` [@validate], and `pointblank` [@pointblank] focuses on providing data validation tools, allowing users to define customized data quality checks based on the applications. 

The literature on data quality typically focuses on the intrinsic or inherent quality of the data themselves, rather than the data's relationship to any specific data analysis. So for example, if a column in a data table is expecting numerical data, but we observe a character value in one of the entries, then that occurrence would trigger some sort of data quality check. This type of quality check can be triggered without any knowledge of what the data will ultimately be used for. However, for a given analysis, we may require specific aspects of the data to be true because they affect the result being computed. Conversely, certain types of poor quality data may have little impact on the ultimate result of an analysis (e.g. data that are missing completely at random). Defining data quality in terms of what may affect a specific analysis outcome or result has the potential to open new avenues for defining data checks and for building algorithms for optimizing the collection of checks defined for a specific analysis.

# Analysis validation checks {#sec-plan}

Expectations represent our understanding of certain aspects of the analysis and the data, independent of the results of the analysis itself. When observed outcomes deviate from these expectations, analysts often revisit the analysis process to identify potential issues, refine methods, or revise assumptions. Experienced analysts can typically identify issues quickly and correct them on the spot, but they often do so without discussing the underlying reasoning, making it harder for less experienced researchers to learn and master these skills. 

Here, we introduce the concept of **analysis validation checks**, which frame these expectations or assumptions as explicit checks that return a TRUE or FALSE result given the data analyzed at hand. Inspired by the concept of data validation checks (@validate), which are designed to ensure that datasets meet expected formats and quality, analysis validation checks reverse the approach: they validate the assumptions about the data necessary for the analysis to produce the *expected results*, as defined by the analyst. The focus on expected results allows the concept of analysis validation checks to encompass a broad range of checks, such as data quality (i.e. missing data, how the data are structured), data distribution and outliers, bivariate and multivariate relationships between variables, and other contextual information. 

Our proposed analysis validation checks provide insights into an analyst's thought process and offer the following benefits: 

  1. Serve as clear checkpoints to support the replication or application of methods to (new) data by programmatically communicating the requirements or assumptions made of the data;
  3. Align assumptions among researchers from different domain backgrounds who may have different expectations about the data;
  4. Improve analysis transparency, reproducibility, and trustworthiness by externalizing a key part of the analysis process; and
  5. Quantify the effectiveness of analysis checks for predicting the expected outcome (see @sec-method);
  
In addition to the above benefits, the development and publication of analysis checks has the potential to help students, inexperienced analysts, and junior researchers develop the skills needed to diagnose unexpected analysis results for a given type of data because the assumptions made about the data are made transparent. The analysis checks can serve as a basis for new analysts to have conversations about the data they are analyzing and to develop a better understanding of the potential data generation process.


## A Toy Example {#sec-toy}

Consider a 30-day step count experiment in public health. Subjects are instructed to walk at least 8,000 steps each day, with an expected average of 9,000 steps, tracked by a step counter app. After 30 days, we review the data and examine the number of steps taken each day. With data of this nature, we may expect there to be occasional "low" days due to factors such as forgetting to wear their watch or unfavorable weather conditions limiting outdoor activities.  We may also expect "high" days recorded after an outdoor hike or intense workout. Given the requirements of the study, we expect that for a given subject 
the average step count would be between $[8500, 9500]$. 

To diagnose potential reasons why this outcome expectation might fail, we can establish a few analysis validation checks in anticipation of seeing the data. For example, we can check the quantile of the step count, if more than a third of the days fall below 8,000, or more than two-thirds exceed 10,000 steps, this could indicate an excess of low-count or high-count days. Similarly, we may may expect the standard deviation of the step count not to be overly large. These considerations yield the following three analysis validation checks that *fails* when:

-   check 1: the 60% quantile of the observed step counts is greater than 10,000
-   check 2: the 40% quantile of the observed step counts is less than 8,000, and
-   check 3: the standard deviation of the observed step counts exceeds 2,500.

The cutoff values chosen for these checks would presumably be chosen based on prior experience with these kinds of data, but could also be optimized using the method presented in the next section.

```{r}
#| fig-height: 3
#| label: fig-step-count
#| fig-cap: Beeswarm plot of average step counts across 300 simulated 30-day periods. Each point represents the average step count from one simulation. While similar to a boxplot or violin plot, the beeswarm plot also displays the distribution of each individual data point. The orange points indicate instances where the average step count fails outside the [8,500, 9,500] interval, representing an unexpected outcome in this scenario. 
set.seed(1234)
step_count <- tibble(id = 1:300) |> 
  rowwise() |> 
  mutate(
    small_n = rpois(1, 8),
    large_n = rpois(1, 8),
    norm_sd = rlogis(1, 800, scale = 100),
    norm_n = 30 - small_n - large_n,
    step = list(round(c(rnorm(small_n, 6000, 200), 
                        rnorm(large_n, 12000, 200), 
                        rnorm(norm_n, 9000, 300)
    ), 0)),
    mean = mean(step, na.rm = TRUE),
    unexpect = ifelse(between(mean, 8500, 9500), 0, 1),
    `q(step, 0.6) > 10000` = ifelse(quantile(step, 0.6, na.rm = TRUE) > 10000, 1, 0),
    `q(step, 0.4) < 8000` = ifelse(quantile(step, 0.4, na.rm = TRUE) < 8000, 1, 0),
    `sd(step) > 2500` = ifelse(sd(step) > 2500, 1, 0),
  ) |>
  ungroup()

step_count |> 
  ggplot(aes(x = mean, y = 1, color = as.factor(unexpect))) + 
  ggbeeswarm::geom_quasirandom() + 
  scale_y_continuous(breaks = seq(0, 13, 1)) +
  scale_color_brewer(palette = "Dark2") + 
  xlab("Average number of steps") + 
  ylab("") + 
  theme_bw() + 
  theme(legend.position = "none", panel.grid.minor = element_blank(), axis.text.y = element_blank())
```


To simulate this data, three normal distributions are used for the daily step counts: $\mathcal{N}(6000, 200)$ for low days, $\mathcal{N}(12000, 200)$ for high days, and $\mathcal{N}(9000, 300)$ for typical days. The number of low and high days can be simulated from a Poisson distribution with $\lambda = 8$. @fig-step-count displays average step count across 300 simulated 30-day periods.

# Method {#sec-method}

While some checks may be crucial and directly indicate an unexpected outcome, others may be tangential to the problem at hand and not indicate a root cause of an unexpected outcome. In this section, we propose a procedure to measure the effectiveness of checks that, when combined using logical operators, contribute to an unexpected outcome. A small set of independent checks is considered effective if it translates to unexpected outcomes.

The approach relies on the use of simulated datasets that are generated based on the analyst's knowledge and assumptions about the data generation mechanism. Datasets are simulated to have the same structure and characteristics of the observed data that will be analyzed at some point in the future. For each simulated dataset, we can apply a collection of analysis validation checks to the dataset and record which ones were TRUE and which were FALSE. We can also compute the outcome of the analysis to see whether the outcome is unexpected in a given simulated datsets. We can then generate many datasets, each time applying the analysis validation checks and computing the outcome. After generating many datasets, we can relate the patterns in the analysis validation checks to the likely that the outcome will be unexpected in a given dataset. @fig-metric-calc provides an overview of the process. 

```{r}
#| label: fig-metric-calc
#| fig-cap: "[this is the cap and the plot needs polish]"
knitr::include_graphics(here::here("figures", "metric-calc.png"))
```

From the simulated data, the accuracy branch refers to a set of checks' ability to accurately detect unexpected outcomes while minimizing false positives and false negatives. While a false positive can raise caution or skepticism on the data, the presence of both false positives and false negatives suggest that the checks are overly sensitive or lack sensitivity to unexpected outcomes. 

To incorporate the effect of multiple checks on the outcome, a logic regression model [@ruczinski_logic_2003] is fitted to the analysis validation checks. Originally developed for SNP microarray data, logic regression constructs Boolean combinations of binary variables, $\mathcal{L}(X_1, X_2, \cdots, X_n)$, to predict both binary and numerical outcomes. In our case, 

* the outcome, $Y$, is a binary variable indicating whether the result is *unexpected* (1) or *expected* (0),
* $X_1, X_2, \cdots, X_n$ represent the binary analysis validation checks, where each check is labeled as 1 if it fails and 0 if it passes.

For classification problems, the logic regression model is reduced to the following form:

$$E(Y) = \mathcal{L}(X_1, X_2, \cdots, X_n)$$
The logical operations of the binary variables $X_1, X_2, \cdots, X_n$, including AND ($\land$), OR ($\lor$), and NOT ($\neg$) and an example of such logic combination can be $X_1 \text{ AND } (X_2 \text{ OR } X_3)$. Simulated annealing is used to explore the search space by minimizing the mis-classification error. The following 6 moves are permitted during the search (as detailed in Figure 2 of @ruczinski_logic_2003): 1) replacing a leaf node, 2) replacing an operator, 3) growing a branch, 4) pruning a branch, 5) splitting a leaf node, and 6) deleting a leaf node.

Compared to other tree-based methods for binary-binary prediction, the Boolean combinations from the logic regression model produce a tree structure that can be directly interpreted as the possible combination of checks leading to an unexpected outcome, without the need to invert the tree as required in classic tree-based recursive partitioning methods. The logic regression model is then used to predict the analysis result based on the values of checks, and the prediction is compared to the actual analysis result in order to calculate the precision and recall of the checks. 

<!-- -   precision: the proportion of correctly identified unexpected results (true positives) out of all the predicted unexpected results (true positives + false positives) -->
<!-- -   recall: the proportion of correctly identified unexpected results (true positives) out of all the actual unexpected results (true positives + false negatives) -->

While checks may score high on accuracy, they may be less effective at explaining the various reasons behind unexpected results. This could happen if, for example, a set of checks are all tangentially related to the cause of the unexpected results, but none addresses the root cause. It may also occur if the checks are highly correlated with one another, leading to redundancy.

To quantify redundancy, the concept of mutual information is used. Mutual information $I(x, y)$ measures the amount of information shared between two random variables and is defined as the KL-distance $D(p \parallel q)$ between the joint distribution of the two variables and the product of the marginal distributions:

$$I(x,y) = D\big(p(x,y) \parallel p(x)p(y)\big) = \sum_x \sum_y p(x,y) \log \frac{p(x,y)}{p(x)p(y)}$$

This concept extends naturally to multiple variables through total correlation, $C(X_1, X_2, \cdots, X_n)$, which captures redundancy across a set of $n$ variables:

$$C(X_1, X_2, \cdots, X_n) = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} p(x_1, x_2, \cdots, x_n) \log \frac{p(x_1, x_2, \cdots, x_n)}{p(x_1)p(x_2) \cdots p(x_n)}$$

A high mutual information value indicates redundancy among the checks, while a low value suggests that the checks are independent and provides unique information to diagnose the unexpected outcome. To standardize this measure, the total correlation *per observation* is calculated, and an independence score, ranging between 0 and 1, is defined as 1 - mutual information.

To combine precision, recall, and independence into a single metric, we can combine the three scores using the arithmetic mean, harmonic mean, or quadratic mean. The differences among these means are minimal when the three metrics are similar. However, as the differences among the metrics increases, the harmonic mean tends to produce the smallest overall score, as it penalizes low values, while the quadratic mean tends to produce the largest score by rewarding higher values more. For simple interpretation of the score, the arithmetic mean is preferred, while in applications where the difference between precision, recall, and independence need to be penalized or rewarded more, the harmonic and quadratic mean should be considered.

## Toy Example Revisited

Returning to the step count example introduced in @sec-toy, the logic regression model is fitted to the three analysis checks described previously to generate the prediction of the unexpected outcome, whether the average number of step falls within the [8,500, 9,500] interval. The predictions from the logic regression model can be compared with the simulated true outcome for calculating the precision and recall metrics. @fig-logic-reg shows the best-fitting logic regression model as

(quantile(step, 0.6) > 10,000 OR quantile(step, 0.4) < 8,000) AND sd(step) < 2,500

In other words, we would predict an unexpected outcome in the analysis if the standard deviation of the step count is not too large (2,500) and either the 60% quantile of the step count exceeds 10,000 or the 40% quantile of the step count falls below 8,000.

```{r}
#| label: fig-logic-reg
#| fig.width: 8
#| fig.heigh: 2
#| fig-cap: Logic regression model fitted to the three checks. The model suggests the rule (quantile(step, 0.6) > 10,000 OR quantile(step, 0.4) < 8,000) AND (NOT sd(step) > 2,500). The NOT operator applied to `sd(step) > 2,500)` is colored with a black background to distinguish it from other checks.
fit <- step_count |> fit_logic_reg(unexpect, `q(step, 0.6) > 10000`:`sd(step) > 2500`, seed = 1, nleaves = 3)
plot_logicreg(fit)
```

```{r}
fit_regtree <- rpart(unexpect ~ ., data = step_count |> dplyr::select(unexpect:`sd(step) > 2500`), method = "class", 
                     control = rpart.control(cp = 0.01)) 
pred_vec <- predict(fit_regtree) |> as_tibble() |> mutate(.fitted = ifelse(`0` > `1`, 0, 1)) |> dplyr::select(.fitted)
regtree_df <- step_count |> dplyr::select(unexpect) |> bind_cols(pred_vec) |> 
  calc_miscla_rate(unexpect, .fitted) |> 
  mutate(overlapping = multiinformation(step_count[,c(8, 11)])/nrow(step_count), 
         independence = 1 - overlapping) |> 
  calc_metrics(metrics = c("harmonic", "arithmetic"))
```

@tbl-logic-reg presents the calculated precision, recall, and independence for the three individual checks and the check rule found by the logic regression. The harmonic and arithmetic means are included to combine the three measures. The results show that the two checks produced by the logic regression can accurately predict 86.1% cases of all *actual unexpected results* in the simulation data. Furthermore, 43.1% of all *predicted unexpected results* were in fact observed to be unexpected.  

```{r}
#| label: tbl-logic-reg
list(tibble(.fitted = step_count$`q(step, 0.6) > 10000`, unexpect = step_count$unexpect),
     tibble(.fitted = step_count$`q(step, 0.4) < 8000`, unexpect = step_count$unexpect),
     tibble(.fitted = step_count$`sd(step) > 2500`, unexpect = step_count$unexpect)
) |>
  map_dfr(~.x |> calc_miscla_rate(unexpect, .fitted) |> 
            mutate(independence = 1) |> 
            calc_metrics(metrics = c("harmonic", "arithmetic"))) |> 
  mutate(checks = c("check 1: q(step, 0.6) > 10000", "check 2: q(step, 0.4) < 8000", "check 3: sd(step) > 2500")) |>
  bind_rows(augment(fit) |> 
              calc_miscla_rate(unexpect, .fitted) |> 
              calc_independence() |> 
              calc_metrics(metrics = c("harmonic", "arithmetic")) |> 
              mutate(checks = "logic regression: (check 1 OR check 2) AND check 3") |> 
              dplyr::select(-overlapping)) |> 
  bind_rows(regtree_df |>
              dplyr::select(-overlapping) |>
              mutate(checks = "regression tree")) |>
  dplyr::select(checks, precision:arithmetic) |> 
  dplyr::rename(`indep.` = independence) |> 
  kable(digits = 3, 
        caption = "Accuracy (precision and recall) and parsimony (independence) metrics for each individual check and the logic regression check rule. The harmonic and arithmetic means of the three metrics are included to evaluate the quality of the checks in  diagnosing unexpected step counts (more than five days with fewer than 8,000 steps).")
```

For comparison, the regression tree produces a similar prediction to the logic regression, however, we argue that the logic regression tree shown in @fig-logic-reg is more interpretable for our purposes because it provides a direct representation of which combinations of analysis checks lead to unexpected outcomes. The logic regression tree is also directly comparable to other diagnostic techniques, which we discuss further in @sec-discussion.



<!-- * provide interpretation at different scenarios: -->

<!--   1) one test is flagged, the prediction is as expected:  --> <!--   2) multiple tests are flagged, the prediction is unexpected,  --> <!--   3) no test is flagged, the prediction is unexpected,  --> <!--   4) no test is flagged, the prediction is as expected -->

# Application {#sec-pm10-mortality}

In the study of the health effects of outdoor air pollution, one area of interest is the association between short-term, day-to-day changes in particulate matter air pollution and daily mortality counts. Substantial work has been done to study this question and to date, there appears to be strong evidence of an association between particulate matter less than 10 $\mu$g/m$^3$ in aerodynamic diameter (PM10) and daily mortality from all non-accidental causes [@samet2000fine]. For our second example, we use the problem of studying PM10 and mortality along with data from the National Morbidity, Mortality, and Air Pollution Study (NMMAPS) to demonstrate how our analysis validation checks described in @sec-method can be applied.

The typical approach to studying the association between PM10 and mortality is to apply a generalized linear model with a Poisson link to relate daily mortality counts to daily measures of PM10. Based on previous work and the range of effect sizes published in the literature, an analyst might expect the coefficient for PM10 in this GLM to lie between $[0, 0.005]$, after adjusting for daily temperature [@samet2000fine; @welty2005acute]. Observing an estimated coefficient outside of this interval would be highly unusual and would warrant a serious re-examination of the analysis process. Therefore, our unexpected outcome in this analysis will be the binary indicator of whether the estimated coefficient for PM10 in the GLM lies outside of the interval $[0, 0.005]$. In addition to providing a more substantial problem for our methods, this example also demonstrates how the procedure presented in @sec-method can be used to select cutoff values in the analysis checks to diagnose an unexpected PM10 coefficient from the generalized linear model. 

This PM10 coefficient expectation can be framed as an analysis check that fails, labelled as 1, if the estimate of the PM10 coefficient (adjusted for temperature) is outside the range [0, 0.005], and 0 if it is within. Multiple factors can affect the estimated PM10 coefficient, such as the sample size, the strength of the correlation between mortality and PM10, and the strength of the correlation between mortality and temperature. Analysts may expect a reasonable sample size to ensure the reliability of the coefficient estimate. Outliers in the three variables can also leverage the coefficient. While these are possible factors that could affect the analysis result, it is not clear what the cutoff values for these checks should be to determine a failure. Here we consider a list of checks in @tbl-checks with varied cutoff values for each: 

| the check fails if ... | 
|--------------| 
| Sample size less than or equal to 200 |
| Sample size less than or equal to 400 |
| Sample size less than or equal to 600 |
| Sample size less than or equal to 800 |
| Mortality-PM10 correlation less than -0.03 |
| Mortality-PM10 correlation less than -0.04 |
| Mortality-PM10 correlation less than -0.05 |
| Mortality-PM10 correlation less than -0.06 |
| Mortality-temperature correlation greater than -0.3 | 
| Mortality-temperature correlation greater than -0.35 | 
| Mortality-temperature correlation greater than -0.4 | 
| Mortality-temperature correlation greater than -0.45 | 
| Outlier(s) are presented in the variable PM10 | 
| Outlier(s) are presented in the variable mortality | 

: A list of checks considered for the generalized linear model of mortality on PM10 and temperature. The checks are based on the sample size, correlation between mortality and PM10, correlation between mortality and temperature, and univariate outlier detection. Multiple cutoff values are specified for each check to determine a failure. {#tbl-checks} 


## Data Simulation

To generate replicates of the dataset, we first generate the correlation matrix of the three variables (PM10, mortality, and temperature) in a grid and then use a Gaussian copula to generate a multivariate normal distribution based on the specified correlation matrix and sample size. The multivariate normal distribution is transformed using the normal CDF before the inverse CDF of the assumed distributions of the three variables is applied. To determine the appropriate distribution of each variable, various distributions are fitted and compared. This includes poisson and negative binomial for mortality; gamma, log-normal, exponential, weibull, and normal for PM10 and temperature; and beta for PM10 after rescaling the data to $[0,1]$. 

To ensure a reasonable likeness to data that might be used in such an analysis, we use characteristics of the observed dataset to refine our simulations. AIC is used to determine the best distribution fit for each variable with the QQ-plot presented in @fig-dist-fit to evaluate the fit. AIC suggests a negative binomial distribution for mortality, a beta distribution for PM10 (multiple by 100 to recover the original scale), and a Weibull distribution for temperature. To include the potential effect of outliers, we add a single outlier to the data for both the mortality and PM10 variables.

```{r}
#| label: fig-dist-fit
#| fig-cap: QQ-plot of the distribution fit for mortality, PM10, and temperature based on the fitted distribution from the original data. The fitted distribution is compared to the observed data to assess the distribution fit. 
#| fig.height: 4
#| fig.width: 10
aa2 <- fitdistrplus::fitdist(pm10$mortality, "nbinom")
p1 <- tibble(fitted= rnbinom(558, size = aa2$estimate[1], mu = aa2$estimate[2]),
       observed= pm10$mortality) %>%
  ggplot(aes(sample = fitted)) +
  stat_qq() +
  stat_qq_line() + 
  theme_bw() + 
  theme(aspect.ratio = 1) + 
  ggtitle("mortality: \nNB(size = 74, mu = 183)")


aa6 <- fitdistrplus::fitdist(pm10$pm10[pm10$pm10 < 90]/100, "beta")
p2 <- tibble(fitted = rbeta(557, shape1 = aa6$estimate[1], shape2 = aa6$estimate[2]) * 100,
       observed = pm10$pm10[pm10$pm10 < 90]) %>%
  ggplot(aes(sample = fitted)) +
  stat_qq() +
  stat_qq_line() + 
  theme_bw() + 
  theme(aspect.ratio = 1) + 
  ggtitle("pm10: \n100 * beta(shp1 = 4.21, shp2 = 11.67)")
  

aa4 <- fitdistrplus::fitdist(pm10$temp, "weibull")
p3 <- tibble(fitted= rweibull(558, aa4$estimate[1], aa4$estimate[2]),
       observed= pm10$temp) %>%
  ggplot(aes(sample = fitted)) +
  stat_qq() +
  stat_qq_line() + 
  theme_bw() + 
  theme(aspect.ratio = 1) + 
  ggtitle("temperature: \nweibull(shape = 3.8, scale = 61)")

p1 | p2 | p3
```

```{r}
# cor(pm10$pm10, pm10$mortality)
# cor(pm10$temp, pm10$mortality)
# cor(pm10$temp, pm10$pm10)
corr_grid <- expand.grid(seq(0.01, 0.2, by = 0.02),                         
                         seq(0.25, 0.5, by = 0.05),                           
                         seq(0.25, 0.45, by = 0.05))  

gen_corr_mtx <- function(r1, r2, r3) {   
  # correlation between mortality and pm10 are negative - r1
  # correlation between mortality and temp are negative - r2
  cor_matrix <- matrix(c(1, -r1, -r2,                          
                         -r1, 1, r3,                          
                         -r2, r3, 1), nrow = 3, byrow = TRUE)          
  if (all(eigen(cor_matrix)$values > 0)) return(cor_matrix) 
  }  

corr_mtx <- lapply(1:nrow(corr_grid), function(i) {   
  gen_corr_mtx(corr_grid[i, 1], corr_grid[i, 2], corr_grid[i, 3]) }) 
corr_mtx <- corr_mtx[map_lgl(corr_mtx, ~!is.null(.x))]  
sample_size <- c(200, 400, 600, 800, 1000)
outlier <- c(TRUE, FALSE)  

generate_data <- function(n, mtx, seed = 123, outlier = FALSE) {   
  mu <- c(0, 0, 0)   
  set.seed(seed)  
  data <- mvrnorm(n, mu, mtx, empirical = TRUE)   
  U <- pnorm(data, mean = 0, sd = 1)   
  
  if (!outlier) {
   tibble(mortality = qnbinom(U[,1], size = 74, mu = 183),           
         pm10 = qbeta(U[,2], shape1 = 4.21, shape2 = 11.67) * 100,           
         temp = qweibull(U[,3], shape = 3.8, scale = 61))    
  } else{
    pm10_vec <- qbeta(U[,2], shape1 = 4.21, shape2 = 11.67) * 100
    tibble(
      mortality = c(qnbinom(U[,1], size = 74, mu = 183)[-1],
                    rnorm(n = 1, mean = 300, sd = 10)),           
      pm10 = c(pm10_vec[-1], rnorm(n = 1, mean = 100, sd = 10)),           
      temp = qweibull(U[,3], shape = 3.8, scale = 61)
      )    
  }
  
}

res <- tibble(corr_mtx = corr_mtx) |> 
  mutate(id = row_number()) |> 
  crossing(sample_size, outlier) |> 
  rowwise() |>   
  mutate(data = list(generate_data(n = sample_size, mtx = corr_mtx, outlier = outlier)),          
         fit = list(summary(glm(mortality ~ pm10 + temp, family = "poisson", data))$coefficients))  

pm10_df <- res |>   
  mutate(     
    p_value = fit[2,4],     
    coef = fit[2,1],     
    mor_pm10_correlation = corr_mtx[1, 2],     
    mor_temp_correlation = corr_mtx[1, 3],     
    temp_pm10_yz_correlation = corr_mtx[2, 3],     
    unexpect = ifelse(between(coef, 0, 0.005), 0, 1),
    `smpl_size <= 200` = ifelse(sample_size <= 200, 1, 0),
    `smpl_size <= 400` = ifelse(sample_size <= 400, 1, 0),
    `smpl_size <= 600` = ifelse(sample_size <= 600, 1, 0),
    `smpl_size <= 800` = ifelse(sample_size <= 800, 1, 0),
    `cor(m, PM10) < -0.03` = ifelse(mor_pm10_correlation < -0.03, 1, 0),
    `cor(m, PM10) < -0.04` = ifelse(mor_pm10_correlation < -0.04, 1, 0),
    `cor(m, PM10) < -0.05` = ifelse(mor_pm10_correlation < -0.05, 1, 0),
    `cor(m, PM10) < -0.06` = ifelse(mor_pm10_correlation < -0.06, 1, 0),
    `cor(m, tmp) > -0.3` = ifelse(mor_temp_correlation > -0.3, 1, 0),
    `cor(m, tmp) > -0.35` = ifelse(mor_temp_correlation > -0.35, 1, 0),
    `cor(m, tmp) > -0.4` = ifelse(mor_temp_correlation > -0.4, 1, 0),
    `cor(m, tmp) > -0.45` = ifelse(mor_temp_correlation > -0.45, 1, 0)
  ) |>    
  ungroup() 

pm10_df <- pm10_df |> 
  rowwise() |> 
  mutate(`PM10 outlier` = ifelse(any(scale(data$pm10) > 4), 1, 0),
         `mortality_outlier` = ifelse(any(scale(data$mortality) > 4), 1, 0)
         ) |> 
  ungroup()
```

A logic regression is fitted using all variations of the checks in @tbl-checks to predict whether the PM10 coefficient is unexpected. @fig-linear-reg-tree shows the optimal logic regression tree from the fitted model. Precision, recall, and independence score, along with their harmonic and arithmetic mean are calculated in @tbl-linear-reg. 


::: {#fig-linear-reg-tree}

```{r}
#| fig.width: 9
#| fig.height: 4
fit <- pm10_df |> fit_logic_reg(unexpect, `smpl_size <= 200`:`mortality_outlier`, 
                                seed = 6, nleaves = 4)
plot_logicreg(fit) 
```

Logic regression model fitted to the fourteen checks and the outcome expectation (unexpected) as the response variable. The model suggests the relationship: (NOT sample size less than or equal to 200 AND mortality-PM10 correlation < -0.04) AND (mortality-temperature correlation > -0.35 OR there exist mortality outlier) to predict the unexpected PM10 coefficient. The NOT operator applied to `smpl_size <= 200` is colored with a black background to distinguish it from other checks.
:::



```{r}
#| label: tbl-linear-reg
library(kableExtra)
df <- list(
  tibble(.fitted = pm10_df$`smpl_size <= 200`, 
         unexpect = pm10_df$unexpect),
  tibble(.fitted = pm10_df$`cor(m, PM10) < -0.03`, 
         unexpect = pm10_df$unexpect),
  tibble(.fitted = pm10_df$`cor(m, tmp) > -0.35`, 
         unexpect = pm10_df$unexpect),
  tibble(.fitted = pm10_df$mortality_outlier, 
         unexpect = pm10_df$unexpect),
  augment(fit)) |>
  map_dfr(~.x |> calc_miscla_rate(unexpect, .fitted) |> 
            mutate(overlapping = 0, independence = 1) |> 
            calc_metrics(metrics = c("harmonic", "arithmetic")), .id = "id") 

tbl <- tibble(check = c("check 1: sample size <= 200", "check 2: cor(m, PM10) < -0.03",
                        "check 3: cor(m, tmp) > -0.35", "check 4: mortality_outlier", 
                        "logic regression: (not check 1 AND check 2) AND (check 3 OR check 4)")) |> 
  bind_cols(df |> dplyr::select(-id, -overlapping)) |> rename(`indep.` = independence) 

tbl |> 
  kable(digits = 3, escape = FALSE,
        caption = "Accuracy (precision and recall) and parsimony (independence) metrics derived from the logic regression model, along with harmonic and arithmetic means, for individual checks (1: sample size, 2: mortality-PM10 correlation, 3: mortality-temperature correlation,  4: mortality outlier), and the combined check rule 5: (sample size AND mortality-PM10 correlation) AND (mortality-temperature correlation OR mortality outlier.") 

```

As indicated in @fig-linear-reg-tree, the logic regression model picks up the following cutoff values for each type of check:

-   sample size *larger than* 200
-   mortality-temperature correlation greater than $-0.35$
-   mortality-PM10 correlation less than $-0.04$
-   mortality data contains outliers that are detected by the univariate outlier detection

<!-- (mortality-temperature correlation > -0.35 OR there exist mortality outlier) and (mortality-PM10 correlation < -0.04 AND (not sample size less than 200) )  -->
The tree structure suggests checking mortality-PM10 correlation and a sample size larger than 200 with an additional check of either outlier on mortality or correlation between mortality and temperature. This combined check rule generates a precision of `r round(as.numeric(df[5, "precision"]), 3)` and a recall of `r round(as.numeric(df[5, "recall"]), 3)` for predicting the unexpected PM10 coefficient. The single check, `cor(m, PM10) < -0.03`, is also powerful with a high precision of `r round(as.numeric(df[2, "precision"]), 3)`, but the low recall value of `r round(as.numeric(df[2, "recall"]), 3)` suggests its high false positive rate, as compared to the combined rule suggested by the logic regression. 

As shown in @fig-linear-reg-tree, there is no single analysis check in the tree that predicts an unexpected outcome. rather at least three checks in the tree must be TRUE in order for the model to predict an unexpected outcome. Given the high independence of the checks (@tbl-linear-reg), this suggests that unexpected results are only likely after multiple anomalies are observed in the data.

# Discussion {#sec-discussion}

In this paper we have developed an approach to using analysis validation checks to externalize the assumptions about the data and analysis tools made during the data analysis process. These checks can serve as a useful summary of the analyst's thought process and can describe how characteristics of the data may lead to unexpected outcomes. Using logic regression, we can develop a graphical summary of the analysis validation checks as well as use the logic regression fitting process to choose the optimal set of checks. The logic regression model can also be used to develop summaries of the precision and recall of the collection of analysis validation checks in predicting the likelihood of an unexpected outcome. We demonstrated our method on an example relating daily mortality to outdoor air pollution data.

An interesting connection can be drawn between our logic regression trees and a tool used in systems engineering known as a fault tree. A fault tree is used for conducting a structured risk assessment and has a long history in aviation, aerospace, and nuclear power applications [@vesely1981fault]. A fault tree is a graphical tool that describes the possible combinations of causes and effects that lead to an anomaly. At the top of the tree is a description of an anomaly. The subsequent branches of the tree below the top event indicate possible causes of the event immediately above it in the tree. The tree can then be built recursively until we reach a root cause that cannot be further investigated. Each level of the tree is connected together using logic gates such as AND and OR gates. The leaf nodes of the tree indicate the root causes that may lead to an anomaly. While the logic regression tress are not identical to fault trees, they share many properties, such as the tree-based structure and the indicator of root causes at the leaf nodes. Perhaps more critically, both serve as graphical summaries of the assumptions made in a problem and the specific violations of those assumptions that could lead to an unexpected result. While fault trees are often used to discover the root cause of an anomaly after it occurs, an important use case for fault trees is to develop a comprehensive understanding of a system *before* an anomaly occurs [@michael2002fault].

Visualization methods are also valuable tools for assessing data assumptions and can potentially be formalized as analysis validation checks. For instance, plotting a variable's distribution using a histogram, density plot, or bee swarm plot can reveal outliers or deviations from normality. These visualizations could be re-framed as checks that fail when the data does not conform to the expected distribution. However, translating visualization results into binary checks that return (TRUE, FALSE) remains an open challenge, requiring either manual verification or the development of automated methods to interpret visualization outputs. An existing example of visual test is the R package `vdiffr` [@vdiffr] for software unit testing. The package saves a template plot and compares it to the current plot to determine whether the unit tests passes or fails.

Systematically generating simulated data is a key component of our approach. In the PM10 example, the inverse transform method is used to preserve the correlation structure among mortality, PM10, and temperature. However, simulation can become complex when additional restrictions are imposed. In such cases, techniques like the acceptance-reject method or permutation may be used to generate the data. 

Analysis validation checks are closely related to the concept of unit testing in software engineering. While unit tests isolate and test specific lines of the code, analysis validation checks focus on the assumptions underlying the analysis rather than the explicit code itself. Moreover, while software testing is deterministic, with clear rules for determining failure, analysis validation checks are probabilistic. As a result, an analysis may fail several assumption checks yet produce an expected outcome, or pass all checks but yield an unexpected result.

# Acknowledgement

The article is created using Quarto [@Allaire_Quarto_2022] in R [@R].
The source code for reproducing the work reported in this paper can be
found at: <https://github.com/huizezhang-sherry/paper-avc>. 

# References
