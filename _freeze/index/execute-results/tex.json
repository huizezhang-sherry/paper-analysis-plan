{
  "hash": "84b7e026b00c925a303c3fbc85cb027f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"My wonderful paper\"\nabstract: \"Unexpected results in data analysis can prompt researchers to examine data quality and analysis steps. While some researchers can typically diagnose issues effectively with a few checks, others may struggle to identify appropriate checks to diagnose the problem. [an example of check] These checks are often informal and difficult to trace and discuss in publications, resulting in others questioning the trustworthiness of the analysis. To address this, we formalize the informal checks into an *analysis plan* that encompasses the analysis steps and (the unit tests): one test for whether the result meets expectations and multiple tests for checking the analysis. We then present a procedure to assess the quality of these unit tests based on their accuracy and redundancy on simulated versions of the original data. The accuracy is assessed using binary classification metrics, *i.e.*, precision and recall, while redundancy is calculated using mutual information. This procedure can be used to conduct a sensitivity analysis, compare different analysis plans, and to identify the optimal cutoff point for the unit tests.\"\ncite-method: natbib\nbibliography: references.bib\ndocumentclass: jds\nformat:\n  pdf:\n   keep-tex: true\n   toc: true\npreamble: >\n  \\usepackage{amsfonts,amsmath,amssymb,amsthm}\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\nknitr::opts_chunk$set(echo = FALSE)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.1\nv ggplot2   3.5.1     v tibble    3.2.1\nv lubridate 1.9.3     v tidyr     1.3.1\nv purrr     1.0.2     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(patchwork)\nlibrary(MASS)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(ggh4x)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'ggh4x'\n\nThe following object is masked from 'package:ggplot2':\n\n    guide_axis_logticks\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\npm10 <- read_csv(here::here(\"data\", \"pm10.csv\")) |> \n  filter(!is.na(pm10)) |>    \n  mutate(season = as.factor(season))  \n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 3288 Columns: 6\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (1): season\ndbl  (4): mortality, pm10, temp, dewpt\ndate (1): date\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n\n\\newpage\n\n# Introduction\n\n- emphasize trustworthy data science since it is the theme of the special issue\n\nIn a data analysis, an experienced researcher can often quickly assess whether a result meets their expectation, while others may be unsure of what to anticipate from the analysis or how to recognize when results diverge form expected norms. (These expectations are often based on prior knowledge, domain expertise, or common sense and they can often be framed into a Boolean question that can be answered by a unit test. For example, in a linear regression model, one may expect the p-value of the coefficient to be less than 0.05. On top of the expectation on the final results, researchers may also have expectations on the intermediate steps of the analysis process. For example, one may expect the data doesn't contain outliers, etc. These expectations are often checked early on to prevent an unexpected propagate through the analysis process.)\n\nThis expectation-setting process is often implicit and rarely documented or discussed in publications. (There are most publication requires open source code for publication, however, it is no instruction or guidance on checking ...).  Yet, making them explicit is crucial for 1) helping junior researchers interpret results and diagnose the unexpected, 2) providing checkpoints that support independent replication or application of methods to new data, 3) aligning assumption across researchers from different backgrounds. \n\nIn this paper, we formalize these informal checks into an *analysis plan* comprising the analysis steps along with the associated unit tests. This formalization allows us to examine assumptions made about the data during analysis and to compare different unit tests in diagnosing unexpected outcomes. We then introduce a procedure to assess the quality of these unit tests based on their accuracy and redundancy across simulated version of the original data. The accuracy is assessed using binary classification metrics -- precision and recall -- while redundancy is measured using mutual information. The workflow provides numerical guarantee of the consistency of the expectation and the data for the analysis plan. This means that given the assumption of the data generating mechanism, the analysis will provide the outcome we expect. In data analysis, analysts are not necessarily collecting the data and they can only make reasonable assumption about the quality of the data. This method we propose provides a numerical guarantee, based on simulation, that the analysis will produce the outcome we expect, given the assumption of the data generating mechanism. This provides transparency of the analysis and build trustworthy data analysis.\n\nThe rest of the paper is organized as follows: @sec-plan describes the concept of analysis plan in detail. @sec-examples provides examples of analysis plan [more details]. (need another section here or before examples?) @sec-conclusion concludes the paper.\n\n# Literature review\n\n## Diagnosing unexpected outcomes in data analysis\n\n[@peng_diagnosing_2021] describes three pedagogical exercises of introducing diagnosing unexpected outcome into a classroom setting. \n\nTODO: what if the expectation is \"wrong\"\n\n## Data analysis checks in statistical software\n  \nCurrently, little has been done on how to computationally incorporate this diagnostic approach into data analysis workflow or software. Most of the diagnostic tools focus on defining user-specified rules, such as data quality checks or producing metrics to summarize model performance, as in model diagnostics. For example, the `assertr` package [@assertr] and the `pointblank` package [@pointblank] provide data validation tools to allow users to define data quality checks. In contrast, packages provide model checks tools like `performance` [@performance] and `yardstick` [@yardstick], from the `tidymodels` [@tidymodels] ecosystem, offers goodness-of-fit, residual analysis, and model performance metrics. \n\nThese packages provide the tools to conduct diagnostics but still don't reflect the mental process of how data analysts diagnose the unexpected output. For example, when an unexpected output occurs, an analyst may check on whether a column in the data frame is between two values for data quality. However, it is not documented what motivates the analyst to conduct this check -- whether it also applies to other researchers analyzing new data in the same contexts, whether it is a common practice in the field, or whether it is a reaction to this particular data or scenario. Currently, most of these assumptions are not discussed in the publication or captured by tools themselves. While one might be able to infer some of the mental process of the analysts from external sources, such as talking to them or watching screencast videos produced by the analysts e.g. TidyTuesday screencast videos, these insights are not systematically documented or made machine-readable. This gap highlights the need for tools that provide higher level documentation of reasoning behind the checks, facilitating a more transparent and interpretable analysis process. \n\n## Unit tests\n\nWhy develop unit tests in software engineering? Because it is difficult to predict the outcome of a program, especially when the program is complex. Because the loss associated with a program failure is costly. \n\nExisting tools for checks in data analysis including those check for data quality and model diagnostics. In software development, \n\n* the `testthat` package [@testthat] provides the infrastructure for testing R code. It allows users to write unit tests for functions and packages.\n* The `assertthat` package [@assertthat] helps to write assertion statements that are supposed to be true at a certain point in the code for defensive programming.  \n\n# Analysis plan {#sec-plan}\n\n## Framing checks into unit tests\n\nQ: Whether we should formulate these concept with math notation?? A: only if it helps\n\n<!-- The analysis plan described in this paper should be differentiated from the pre-specifies analysis plan document often used in biostatistics to specifies the hypothesis, data collection mechanism, statistical procedures etc of randomized experiments.  -->\n\nAn analysis plan is a set of analysis steps combined with expectations. Expectations represent our belief about certain aspects of the analysis, independent of the analysis itself. It can be divided into two types: *outcome expectation* and *plan expectation*. Outcome expectation refers to what we anticipate from the main result of the analysis based on prior knowledge. They shape how we interpret the results and assess whether they are consistent with existing knowledge or indicate the need for updates [@grolemund_cognitive_2014]. For example, in public health, prior research shows the average increase in mortality rate per unit increase in PM10 is about 0.50% [@liu2019ambient]. This serves as an expectation for similar future studies. Plan expectations concern the intermediate steps within the analysis rather than the final outcome. They serve as checkpoints to detect deflection in the analysis process For example, we may expect temporal data to be ordered with no gaps and duplicates, or expect that temperature will be a significant covariate in the linear regression model of PM10 on mortality.\n\nExperienced analysts often have implicit expectation about the outcome and rely on a few \"directional signs\" to check when the outcome deviate from those expectation. However, these expectations are rarely made explicit within the analysis workflow. This makes it challenging for consumers of  the analysis to evaluate the results, since it becomes difficult to disentangle whether discrepancies arise from differing expectations or from the use of statistical technique, without running the analysis themselves. Non-expert analysts, lacking prior knowledge or instinct, may not have clear expectations of the results. This can lead to reduced confidence of the analysis and makes it more difficult and time-consuming to diagnose the cause of the deviation when the results don't align with expectations. By explicitly formulating these expectations, an analysis plan can guide the analysis process, facilitate the communication and evaluate the validity of the results.\n\n<!-- The expectations can be thought of as a set of unit tests used to validate the results of data analysis. By specifying a range of values for these tests, multiple versions of the dataset can be generated to satisfy different sets of plan expectations. This allows us to present what we called the \"result universe\" -- the complete set of possible results that can be obtained from one data analysis process. By visualizing the result universe, data analysis consumers can observe how changes in expectations affect the results and the range of alternative outcomes that could arise under different conditions. This enables them to evaluate the outcomes based on their own plan expectations and gain a broader perspective on how the actual results produced by analysts fit within this spectrum of possibilities, promoting transparency and trust in analysis.   -->\n\n## A toy example\n\nConsider a 30-day step count goal. Suppose you resolve to walk at least 8,000 steps each day, using an app to record your daily step count. Your target average is 9,000 steps per day, with some \"lows\" day, where you walk around 4,000 steps and \"high\" days  where you reach about 12,000 steps. After 30 days, you check how many times your step count fall below 8,000 and aim for no more than five days under this threshold. \n\nTo simulation this data, three normal distributions with different means are used for the daily step counts: $\\mathcal{N}(4000, 200)$ for low days, $\\mathcal{N}(12000, 200)$ for high days, and $\\mathcal{N}(9000, 300)$ for typical days. The number of low and high days can be simulated from a Poisson distribution with $\\lambda = 4$. @fig-step-count displays the number of days with fewer than 8,000 steps across 300 simulated 30-day periods.\n\nIn this scenario, the outcome expectation is that the number of days with a step count below 8,000 will be no more than five. To diagnose potential reasons why this outcome expectation might fail, we can establish a few plan expectations. For example, if the average step count is too low, this may suggest there are too many low days, potentially lead to an unexpected outcome. Similarly, we can also check the quantile of the step count, if more than a third of the days fall below 8,000, this could indicate an excess of low-count days. Additionally, we may may expect the standard deviation of the step count not to be overly large. \n\nThese considerations yield the following three unit tests as plan expectations:\n\n  - test1: the test fails if the mean step count is below 8,200\n  - test2: the test fails if the 30th percentile of the step counts is below 8,200 \n  - test3: the test fails if the standard deviation of the step countsexceeds 2,500. \n\n\n\n::: {#cell-fig-step-count .cell}\n\n```{.r .cell-code .hidden}\nset.seed(1234)\nstep_count <- tibble(id = 1:300) |> \n  rowwise() |> \n  mutate(\n    small_n = rpois(1, 4),\n    large_n = rpois(1, 4),\n    norm_sd = rlogis(1, 800, scale = 100),\n    norm_n = 30 - small_n - large_n,\n    step = list(round(c(rnorm(small_n, 4000, 200), \n                        rnorm(large_n, 12000, 200), \n                        rnorm(norm_n, 9000, 300)\n                        ), 0)),\n    less_than_8000 = sum(step < 8000), \n    unexpect = ifelse(less_than_8000 > 5, 1, 0),\n    test1 = ifelse(mean(step) < 8200, 1, 0),\n    test2 = ifelse(quantile(step, 0.3, na.rm = TRUE) < 8300, 1, 0),\n    test3 = ifelse(sd(step) > 2500, 1, 0),\n    ) |>\n  ungroup()\n\n# step_count |> \n#   dplyr::select(id, step) |> \n#   unnest(step) |>\n#   mutate(day = rep(1:30, 300)) |> \n#   ggplot(aes(x = step, y = id)) + \n#   geom_point(alpha = 0.5) + \n#   theme_bw()\n\nstep_count |> \n  ggplot(aes(x = less_than_8000, fill = as.factor(unexpect))) + \n  geom_histogram(binwidth = 0.5) + \n  scale_x_continuous(breaks = seq(0, 13, 1)) +\n  scale_fill_brewer(palette = \"Dark2\") + \n  xlab(\"Number of days with step count less than 8000\") + \n  theme_bw() + \n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![Number of days with fewer than 8,000 steps across 300 simulated 30-day periods. The orange bars indicate instances where the count exeeds five days, representing an unexpected outcome in this scenario.](index_files/figure-pdf/fig-step-count-1.pdf){#fig-step-count fig-pos='H'}\n:::\n:::\n\n\n\n# Method \n\n## A workflow to assess the quality of unit tests\n\nIn real-world applications, it is rare to create a set of unit tests can fully guarantee expected results. On one hand, there is the cost of effort involved in manually developing all these tests; on the other, there is the inherent complexity of the problem. (can we - if we're thriving for detecting 95% of the cause?). However, the quality of unit tests can be evaluated by simulated data. One set of tests is considered better than another if a small set of tests can reliably detect unexpected outcomes, which brings two criteria in the evaluation metric: accuracy and parsimony.\n\nAccuracy refers to a set of tests' ability to accurately detect unexpected outcomes while minimizing false positives and false negatives. A false positive can indicate (caution or skepticism on checking the data), whereas a false negative suggests the tests may lack sensitivity to unexpected outcomes. Since both plan and outcome expectations can be framed as unit tests with binary outcomes, one approach is to predict the outcome expectation based on the plan expectation. Performance of the tests can then be evaluated using precision and recall metrics from the confusion matrix: \n\n  * precision: the proportion of correctly identified unexpected results (true positives) out of all the predicted unexpected results (true positives + false positives)\n  * recall: the proportion of correctly identified unexpected results (true positives) out of all the actual unexpected results (true positives + false negatives)\n\nThe second criteria is parsimony in the tests. While tests may score high on accuracy, they may be less effective at explaining the reasons behind unexpected results. This could happen if a set of tests are all tangentially related to the cause of the unexpected results, but none addressing the root cause. It may also occur if the tests are correlated with one another, leading to redundancy.\n\n\n- explain mutual information\n- explain logic regression \n- explain combining precision, recall, and independence together through \"means\"\n\n* A logic regression (ref) is used to model the relationship between the plan and outcome expectations. (justify the use of Logic regression)\n\n\n## Toy example revisited \n\n* provide interpretation at different scenarios:\n    \n  1) one test is flagged, the prediction is as expected: \n  2) multiple tests are flagged, the prediction is unexpected, \n  3) no test is flagged, the prediction is unexpected, \n  4) no test is flagged, the prediction is as expected\n\n\n# Applications {#sec-examples}\n\nThree examples are presented to illustrate how the concept of analysis plan can be applied to data analysis. [toy example]. @sec-linear-reg illustrates how constructing  the result universe in a linear regression model of PM10 on mortality can help understand the impact of sample size, model specification, and variable correlation structure on data analysis. [example three]\n\n\n## Linear regression {#sec-linear-reg}\n\nConsider a linear regression model to study the effect of PM10 on mortality (provide context of using PM10 to study mortality). Analysts may expect a significant (p-value $\\le$ 0.05) PM10 coefficient in the linear model from the literature. This is the *outcome expectation*. There are multiple factors that can affect the outcome expectation of linear regression, which here is called *plan expectation*, for example, 1) sample size, 2) model specification, and 3) correlation structure between variables. Adequate sample size is required to achieve the desired power to detect the significance of PM10 on mortality. Temperature is often an important confounder to consider in such study (add reference). From some domain knowledge, an analyst may expect that the significance of PM10 coefficient can be attained by adding temperature to the model. Analysts may also expect certain correlation structure between PM10, temperature, and mortality, and the distribution of each variable. \n\n * add a paragraph to describe the simulation process\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Generate correlation matrices \ncorr_grid <- expand.grid(seq(-0.01, -0.001, by = 0.001), \n                         seq(-0.6, -0.2, 0.05), seq(0.2, 0.6, 0.05))\n\n# Function to compute correlation matrix for each combination \ngen_corr_mtx <- function(r1, r2, r3) {   \n  cor_matrix <- matrix(c(1, r1, r2,                          \n                         r1, 1, r3,                          \n                         r2, r3, 1), nrow = 3, byrow = TRUE)          \n  # Store the matrix in the list   \n  if (all(eigen(cor_matrix)$values > 0)) return(cor_matrix) \n  }  \n\n# Plan for parallel processing\ncorr_mtx <- lapply(1:nrow(corr_grid), function(i) {   \n  gen_corr_mtx(corr_grid[i, 1], corr_grid[i, 2], corr_grid[i, 3]) \n  }) \ncorr_mtx <- corr_mtx[map_lgl(corr_mtx, ~!is.null(.x))]  \nsample_size <- c(50, 100, 500, 1000) \nmodel <- c(\"mortality ~ pm10 + temp\", \"mortality ~ pm10\") |> map(as.formula) \n\ngenerate_data <- function(n, mtx, seed = 123) {   \n  mu <- c(0, 0, 0)   \n  data <- mvrnorm(n, mu, mtx, empirical = TRUE)   \n  U <- pnorm(data, mean = 0, sd = 1)   \n  set.seed(seed)      \n  tibble(mortality = qpois(U[,1], 182), # assume distribution\n         pm10 = qgamma(U[,2], shape = 6, rate = 0.2),           \n         temp = qnorm(U[,3], mean = 55, sd = 16))   \n  }  \n\nres <- tibble(corr_mtx = corr_mtx) |> \n  mutate(id = row_number()) |> \n  crossing(sample_size, model) |> \n  rowwise() |>   \n  mutate(data = list(generate_data(n = sample_size, mtx = corr_mtx)),          \n         fit = list(summary(lm(model, data))$coefficients))  \n```\n:::\n\n\n\n# Discussion\n\n* how to systematically simulate data is still unknown, sensitivity of the simulation to the results \n\n* plotting is a critical way to check data and they can still be frame into a unit test. it is a open problem to how to encode the visualization into the unit tests. Maybe a procedure like confirm plot (this looks alright to you) and then press the button to continue\n\n* currently no automated way to generate unit tests. It is interesting to see the automation of generating unit tests, although it requires the inputs from experts across a wide array of common scenarios.  \n\n* There are cost and benefit on setting expectation on different granularity. At the lowest level, one may have a plan for each data entry and every data handling steps. This requires more work from the analysts and may not be practical in practice. For more complex analyses, analysts may divide the analysis into sections and set expectations for each. They can then focus on the specific sections flagged by the tests and sub-divide the sections to set expectation and diagnose the analysis in a hierarchical manner.\n\n* (only mention it) Software testing relies on \"oracles\" to provide the expected output necessary for verifying test results. For example, to test whether the program correctly calculates 1 + 1, one need to supply the correct answer, 2. However, establishing this \"correct\" output can sometimes be challenging, where obtaining a solution may be difficult without the program itself. This situation leads to the oracle problem [@barr2014oracle]. In data analysis, the similar oracle problem can happen, as the \"truth\" of an outcome, the expectation, depends on the underlying theory or interpretation. For example, in a linear regression model, the significance of a coefficient may be expected or unexpected based on the theory, making it challenging for researcher with a different theory to assess the results and the analysis. \n\n\n# Conclusion {#sec-conclusion}\n\n\n# References\n",
    "supporting": [
      "index_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}