{
  "hash": "8e9d6fbabf6b92494288dfdf583b87ca",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"My wonderful paper\"\nabstract: \"Unexpected results in data analysis can prompt researchers to examine data quality and analysis steps. While some researchers can typically diagnose issues effectively with a few checks, others may struggle to identify appropriate checks to diagnose the problem. [an example of check] These checks are often informal and difficult to trace and discuss in publications, resulting in others questioning the trustworthiness of the analysis. To address this, we formalize the informal checks into an *analysis plan* that encompasses the analysis steps and (the unit tests): one test for whether the result meets expectations and multiple tests for checking the analysis. We then present a procedure to assess the quality of these unit tests based on their accuracy and redundancy on simulated versions of the original data. The accuracy is assessed using binary classification metrics, *i.e.*, precision and recall, while redundancy is calculated using mutual information. This procedure can be used to conduct a sensitivity analysis, compare different analysis plans, and to identify the optimal cutoff point for the unit tests.\"\nauthor:\n  - name: \"H. Sherry Zhang\"\n    affiliation: \"University of Texas at Austin\"\n    email: \"huize.zhang@austin.utexas.edu\"\n    attributes:\n      corresponding: true\n  - name: \"Roger D. Peng\"\n    affiliation: \"University of Texas at Austin\"\n    email: \"roger.peng@austin.utexas.edu\"\ncite-method: natbib\nbibliography: references.bib\nformat:\n tandf-pdf:\n   keep-tex: true\n   fontsize: 12pt\n   linestretch: 2\n   notebook-links: false\npreamble: >\n  \\usepackage{amsfonts,amsmath,amssymb,amsthm}\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\nknitr::opts_chunk$set(echo = FALSE)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.1\nv ggplot2   3.5.1     v tibble    3.2.1\nv lubridate 1.9.3     v tidyr     1.3.1\nv purrr     1.0.2     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(patchwork)\nlibrary(MASS)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(ggh4x)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'ggh4x'\n\nThe following object is masked from 'package:ggplot2':\n\n    guide_axis_logticks\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(adtoolbox)\nlibrary(broom)\nlibrary(knitr)\nlibrary(rpart)\nlibrary(infotheo)\npm10 <- read_csv(here::here(\"data\", \"pm10.csv\")) |> \n  filter(!is.na(pm10)) |>    \n  mutate(season = as.factor(season))  \n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 3288 Columns: 6\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (1): season\ndbl  (4): mortality, pm10, temp, dewpt\ndate (1): date\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n\n\nREADME: \n\n* *Check for TODOs*\n- Literature review: Section *diagnosing unexpected outcomes in data analysis* \n- *Discussion* and *Conclusion*\n- In Section *Application*: provide more context on the PM10-mortality study and add reference of the [0, 0.005] PM10 coefficient\n\n\\newpage\n\n# Introduction\n\n-   TODO: emphasize trustworthy data science since it is the theme of the special issue\n\nIn data analysis, experienced researchers often rely on their prior knowledge or domain expertise to quickly assess whether results align with their expectations. When outcomes deviate, they can quickly iterate through the analysis cycle to identify potential issues, refine methods, or revise assumptions. However, this iterative process they employ is rarely explicit, documented, or discussed in publications. As a result, readers are typically presented with the final outcomes of the analysis cycle where the results and expectations are aligned -- achieved either by refining the analysis or updating the expectations based on statistical evidence. These missing pieces of information provides little guidance for diagnosing issues in the analysis when the same methodology is applied to a new dataset that produces different outcomes. Similarly, when researchers with different background knowledge it becomes unclear whether variations in results arise from differences in the assumptions or in the analysis itself.\n\nIn this paper, we frame these expectations as checks within an *analysis plan*, which allows us to examine these implicit assumptions made about the data during analysis and to compare different checks for diagnosing unexpected outcomes. We then introduce a procedure to evaluate the quality of these checks, either individually or in combination through logic regression, based on their accuracy and redundancy across simulation of the original data. Accuracy is assessed using binary classification metrics, precision and recall, while redundancy is measured using mutual information. The proposed workflow offers a numerical guarantee, based on simulation, that the analysis will produce the expected results, assuming the data generating mechanism holds. [something about trustworthy data science]\n\nThe rest of the paper is organized as follows: @sec-lit-review reviews the concept of diagnosing unexpected outcomes and data quality checks. @sec-plan introduces the concept of analysis plan, illustrated with a toy example on fitness step count. @sec-method describes the procedure for selecting optimal check combinations to predict unexpected outcomes. @sec-pm10-mortality applies this selection procedure to a linear regression example examining the effect of PM10 on mortality. @sec-discussion discusses a few key considerations and @sec-conclusion concludes the paper.\n\n# Literature review {#sec-lit-review}\n\n## Diagnosing unexpected outcomes in data analysis\n\n[@peng_diagnosing_2021] describes three pedagogical exercises of introducing diagnosing unexpected outcome into a classroom setting.\n\nTODO: what if the expectation is \"wrong\"\n\n## Data analysis checks \n\nA substantial body of literature has addressed the definition of data quality [@8642813, more] and developed frameworks, that includes dimensions, attributes, and measures to evaluate and improve data quality [@cai2015challenges; @wang1996beyond; @6204995; @woodall2014classification]. These frameworks are often used information system and database management and support business decision-making in the industry. For research purposes, high-quality data ensures the credibility of scientific findings and supports reproducibility and reusability in future studies [ref]. With the growing prevalence of open data in scientific research, the consumers or users of the data typically are no longer the data producers or collectors who have the full knowledge of data in hand, prompting more interest towards the data quality checks in the data analysis process. In R, there are some packages, like`skimr` [@skimr] and `dataMaid` [@dataMaid], provides basic data screening and reporting, while another class of packages, e.g. `assertr` [@assertr], `validate` [@validate], and `pointblank` [@pointblank], focuses on providing data validation tools, allowing users to define customized data quality checks based on the applications. \n\n# Analysis validation checks {#sec-plan}\n\n## Framing expectations as checks\n\nQ: Whether we should formulate these concept with math notation?? A: only if it helps\n\nWhile data validation focuses on verifying or subsetting data based on predefined rules [@zio2015methodology], data analysis checks specify assumptions made on the data required for the analysis process to obtain the reported results. These checks often involve evaluating aspects such as data distribution and outliers, relationships between variables, and the validity of model assumptions.\n\nHowever, there are other checks that reflect the mental process of of how data analysts diagnose the unexpected output that are not documented in this process. For example, when an unexpected output occurs, an analyst may check on whether a column in the data frame is between two values for data quality. However, it is not documented what motivates the analyst to conduct this check -- whether it also applies to other researchers analyzing new data in the same contexts, whether it is a common practice in the field, or whether it is a reaction to this particular data or scenario. Currently, most of these assumptions are not discussed in the publication or captured by tools themselves. While one might be able to infer some of the mental process of the analysts from external sources, such as talking to them or watching screencast videos produced by the analysts e.g. TidyTuesday screencast videos, these insights are not systematically documented or made machine-readable. A question to consider here is whether it is possible to deliver this type of quality checks as structured data in a data analysis. This gap highlights the need for tools that provide higher level documentation of reasoning behind the checks, facilitating a more transparent and interpretable analysis process.\n\nAn analysis plan is a set of analysis steps combined with expectations. Expectations represent our belief about certain aspects of the analysis, independent of the analysis itself. It can be divided into two types: *outcome expectation* and *plan expectation*. Outcome expectation refers to what we anticipate from the main result of the analysis based on prior knowledge. They shape how we interpret the results and assess whether they are consistent with existing knowledge or indicate the need for updates [@grolemund_cognitive_2014]. For example, in public health, prior research shows the average increase in mortality rate per unit increase in PM10 is about 0.50% [@liu2019ambient]. This serves as an expectation for similar future studies. Plan expectations concern the intermediate steps within the analysis rather than the final outcome. They serve as checkpoints to detect deflection in the analysis process For example, we may expect temporal data to be ordered with no gaps and duplicates, or expect that temperature will be a significant covariate in the linear regression model of PM10 on mortality.\n\nExperienced analysts often have implicit expectation about the outcome and rely on a few \"directional signs\" to check when the outcome deviate from those expectation. However, these expectations are rarely made explicit within the analysis workflow. This makes it challenging for consumers of the analysis to evaluate the results, since it becomes difficult to disentangle whether discrepancies arise from differing expectations or from the use of statistical technique, without running the analysis themselves. Non-expert analysts, lacking prior knowledge or instinct, may not have clear expectations of the results. This can lead to reduced confidence of the analysis and makes it more difficult and time-consuming to diagnose the cause of the deviation when the results don't align with expectations. By explicitly formulating these expectations, an analysis plan can guide the analysis process, facilitate the communication and evaluate the validity of the results.\n\nYet, making them explicit is crucial for \n\n  1) helping junior researchers interpret results and diagnose the unexpected, \n  2) providing checkpoints that support independent replication or application of methods to new data, and\n  3) aligning assumption across researchers from different backgrounds.\n\n<!-- The expectations can be thought of as a set of unit tests used to validate the results of data analysis. By specifying a range of values for these tests, multiple versions of the dataset can be generated to satisfy different sets of plan expectations. This allows us to present what we called the \"result universe\" -- the complete set of possible results that can be obtained from one data analysis process. By visualizing the result universe, data analysis consumers can observe how changes in expectations affect the results and the range of alternative outcomes that could arise under different conditions. This enables them to evaluate the outcomes based on their own plan expectations and gain a broader perspective on how the actual results produced by analysts fit within this spectrum of possibilities, promoting transparency and trust in analysis.   -->\n\n## A toy example\n\nConsider a 30-day step count goal. Suppose you resolve to walk at least 8,000 steps each day, using an app to record your daily step count. Your target average is 9,000 steps per day, with some \"lows\" day, where you walk around 4,000 steps and \"high\" days where you reach about 12,000 steps. After 30 days, you check how many times your step count fall below 8,000 and aim for no more than five days under this threshold.\n\nTo simulation this data, three normal distributions with different means are used for the daily step counts: $\\mathcal{N}(4000, 200)$ for low days, $\\mathcal{N}(12000, 200)$ for high days, and $\\mathcal{N}(9000, 300)$ for typical days. The number of low and high days can be simulated from a Poisson distribution with $\\lambda = 4$. @fig-step-count displays the number of days with fewer than 8,000 steps across 300 simulated 30-day periods.\n\nIn this scenario, the outcome expectation is that the number of days with a step count below 8,000 will be no more than five. To diagnose potential reasons why this outcome expectation might fail, we can establish a few plan expectations. For example, if the average step count is too low, this may suggest there are too many low days, potentially lead to an unexpected outcome. Similarly, we can also check the quantile of the step count, if more than a third of the days fall below 8,000, this could indicate an excess of low-count days. Additionally, we may may expect the standard deviation of the step count not to be overly large.\n\nTODO: add one or two scenarios on why step count could go up or down: 1) you take off your watch in the middle of the day, 2) have an workout\n\nTODO: phrase it as Researcher measuring someone's step count rather than measuring your own step count\n\nThese considerations yield the following three unit tests as plan expectations:\n\n-   test1: the test fails if the mean step count is below 8,200\n-   test2: the test fails if the 30th percentile of the step counts is below 8,200\n-   test3: the test fails if the standard deviation of the step countsexceeds 2,500.\n\n\n\n::: {#cell-fig-step-count .cell}\n\n```{.r .cell-code .hidden}\nset.seed(1234)\nstep_count <- tibble(id = 1:300) |> \n  rowwise() |> \n  mutate(\n    small_n = rpois(1, 4),\n    large_n = rpois(1, 4),\n    norm_sd = rlogis(1, 800, scale = 100),\n    norm_n = 30 - small_n - large_n,\n    step = list(round(c(rnorm(small_n, 4000, 200), \n                        rnorm(large_n, 12000, 200), \n                        rnorm(norm_n, 9000, 300)\n                        ), 0)),\n    less_than_8000 = sum(step < 8000), \n    unexpect = ifelse(less_than_8000 > 5, 1, 0),\n    test1 = ifelse(mean(step) < 8200, 1, 0),\n    test2 = ifelse(quantile(step, 0.3, na.rm = TRUE) < 8300, 1, 0),\n    test3 = ifelse(sd(step) > 2500, 1, 0),\n    ) |>\n  ungroup()\n\n# step_count |> \n#   dplyr::select(id, step) |> \n#   unnest(step) |>\n#   mutate(day = rep(1:30, 300)) |> \n#   ggplot(aes(x = step, y = id)) + \n#   geom_point(alpha = 0.5) + \n#   theme_bw()\n\nstep_count |> \n  ggplot(aes(x = less_than_8000, fill = as.factor(unexpect))) + \n  geom_histogram(binwidth = 0.5) + \n  scale_x_continuous(breaks = seq(0, 13, 1)) +\n  scale_fill_brewer(palette = \"Dark2\") + \n  xlab(\"Number of days with step count less than 8000\") + \n  theme_bw() + \n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![Number of days with fewer than 8,000 steps across 300 simulated 30-day periods. The orange bars indicate instances where the count exeeds five days, representing an unexpected outcome in this scenario.](index_files/figure-pdf/fig-step-count-1.pdf){#fig-step-count fig-pos='H'}\n:::\n:::\n\n\n\n# Method {#sec-method}\n\n## A workflow to assess the quality of unit tests\n\nIn real-world applications, it is rare to create a set of unit tests can fully guarantee expected results. On one hand, there is the cost of effort involved in manually developing all these tests; on the other, there is the inherent complexity of the problem. (can we - if we're thriving for detecting 95% of the cause?). However, the quality of unit tests can be evaluated by simulated data. One set of tests is considered better than another if a small set of tests can reliably detect unexpected outcomes, which brings two criteria in the evaluation metric: accuracy and parsimony. @fig-metric-calc illustrates the workflow for calculating the metrics.\n\n\n\n::: {#cell-fig-metric-calc .cell}\n\n```{.r .cell-code .hidden}\nknitr::include_graphics(here::here(\"figures\", \"metric-calc.png\"))\n```\n\n::: {.cell-output-display}\n![this is the cap](figures/metric-calc.png){#fig-metric-calc fig-pos='H' width=6.73in}\n:::\n:::\n\n\n\nAccuracy refers to a set of tests' ability to accurately detect unexpected outcomes while minimizing false positives and false negatives. A false positive can indicate (caution or skepticism on checking the data), whereas a false negative suggests the tests may lack sensitivity to unexpected outcomes. To model the relationship between the plan and outcome expectation (binary-binary), a logic regression model is used [@ruczinski_logic_2003]. Originally developed for SNP microarray data, logic regression constructs Boolean combinations of binary variables to solve regression problems \\[more introduction on logic regression\\]. Compared to other tree-based methods or machine learning methods for binary-binary prediction, logic regression produces Boolean combinations, or meta rules, that combines unit tests to solve the prediction problem. \\[need rewrite here\\]. The performance of the tests can be evaluated using precision and recall metrics derived from the confusion matrix of the logic regression prediction\n\n-   precision: the proportion of correctly identified unexpected results (true positives) out of all the predicted unexpected results (true positives + false positives)\n-   recall: the proportion of correctly identified unexpected results (true positives) out of all the actual unexpected results (true positives + false negatives)\n\nThe second criteria is parsimony in the tests. While tests may score high on accuracy, they may be less effective at explaining the reasons behind unexpected results. This could happen if a set of tests are all tangentially related to the cause of the unexpected results, but none addressing the root cause. It may also occur if the tests are correlated with one another, leading to redundancy.\n\nTo quantify redundancy, the concept of mutual information is used. Mutual information $I(x, y)$ measures the amount of information shared between two random variables and is defined as the KL-distance $D(p \\parallel q)$ between the joint distribution of the two variables and the product of the marginal distributions:\n\n$$I(x,y) = D\\big(p(x,y) \\parallel p(x)p(y)\\big) = \\sum_x \\sum_y p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)}$$\n\nThis concept extends naturally to multiple variables through total correlation \\[ref\\], $C(X_1, X_2, \\cdots, X_n)$, which captures redundancy across a set of $n$ variables:\n\n$$C(X_1, X_2, \\cdots, X_n) = \\sum_{x_1} \\sum_{x_2} \\cdots \\sum_{x_n} p(x_1, x_2, \\cdots, x_n) \\log \\frac{p(x_1, x_2, \\cdots, x_n)}{p(x_1)p(x_2) \\cdots p(x_n)}$$\n\nA high mutual information value indicates redundancy among the tests, while a low value suggests that the tests are independent and provide unique information to diagnose the unexpected outcome. To standardize this measure, the total correlation *per observation* is calculated, and an independence score, ranging between 0 and 1, is defined as 1 - mutual information.\n\nTo combine precision, recall, and independence into a single metric, various mathematical means, such as arithmetic mean, harmonic mean, and quadratic mean, can be used. The differences among these means are minimal when the three metrics are similar. However, as the differences among the metrics increases, the harmonic mean tends to produce the smallest overall score, as it penalizes low values, while the quadratic mean tends to produce the largest score by rewarding higher values more. For simple interpretation of the score, the arithmetic mean is preferred, while in applications where the difference between precision, recall, and independence need to be penalized or rewarded more, the harmonic and quadratic mean should be considered.\n\n## Toy example revisited\n\nIn the step count example, we can use the logic regression model to evaluate the quality of the unit tests. The logic regression model is fitted to the three unit tests (test1, test2, test3) and the outcome expectation (unexpect) as the response variable. The model is then used to predict the outcome expectation based on the unit tests. The prediction is then compared to the actual outcome expectation to calculate the precision and recall of the tests. The independence of the tests is also calculated to assess the redundancy of the tests. @fig-logic-reg presents the suggested logic regression model as a combination of test 1 and test 3 with an OR operator.\n\n@tbl-logic-reg presents the calculated precision, recall, and independence for the three individual tests and the combined test rule (test1 OR test3) from the logic regression. The harmonic and arithmetic means are included to evaluate the quality of the unit tests in diagnosing unexpected step counts. The results show that the combined test rule (test1 OR test3) has the highest precision, recall, and independence, suggesting that it is the most effective test for diagnosing unexpected step counts. We also include the metric calculated from fitting a regression tree model to the data to compare the performance of the logic regression model. The regression tree produces a similar model of first split on test1 and then split on test3, and results in the same accuracy and overall score as the logic regression model.\n\ntop-down and bottom up (regression tree + logic tree): more naturally useful summary of the say it is organized. (put down the plot)\n\n<!-- * provide interpretation at different scenarios: -->\n\n<!--   1) one test is flagged, the prediction is as expected:  --> <!--   2) multiple tests are flagged, the prediction is unexpected,  --> <!--   3) no test is flagged, the prediction is unexpected,  --> <!--   4) no test is flagged, the prediction is as expected -->\n\n\n\n::: {#cell-fig-logic-reg .cell fig.heigh='2'}\n\n```{.r .cell-code .hidden}\nfit <- step_count |> fit_logic_reg(unexpect, test1:test3, seed = 1, nleaves = 4)\nplot_logicreg(fit)\n```\n\n::: {.cell-output-display}\n![Logic regression model fitted to the three unit tests (test1, test2, test3) and the outcome expectation (unexpect) as the response variable. The model suggests using an OR rule to combine test1 and test3 to predict the outcome expectation.](index_files/figure-pdf/fig-logic-reg-1.pdf){#fig-logic-reg fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nfit_regtree <- rpart(unexpect ~ ., data = step_count |> dplyr::select(unexpect:test3), method = \"class\", \n                     control = rpart.control(cp = 0.01)) \npred_vec <- predict(fit_regtree) |> as_tibble() |> mutate(.fitted = ifelse(`0` > `1`, 0, 1)) |> dplyr::select(.fitted)\nregtree_df <- step_count |> dplyr::select(unexpect) |> bind_cols(pred_vec) |> \n  calc_miscla_rate(unexpect, .fitted) |> \n  mutate(overlapping = multiinformation(step_count[,c(9, 11)])/nrow(step_count), \n         independence = 1 - overlapping) |> \n  calc_metrics(metrics = c(\"harmonic\", \"arithmetic\"))\n```\n:::\n\n::: {#tbl-logic-reg .cell}\n\n```{.r .cell-code .hidden}\nlist(tibble(.fitted = step_count$test1, unexpect = step_count$unexpect),\n     tibble(.fitted = step_count$test2, unexpect = step_count$unexpect),\n     tibble(.fitted = step_count$test3, unexpect = step_count$unexpect)\n) |>\n  map_dfr(~.x |> calc_miscla_rate(unexpect, .fitted) |> \n            mutate(independence = 1) |> \n            calc_metrics(metrics = c(\"harmonic\", \"arithmetic\")), .id = \"tests\") |> \n  mutate(tests = c(\"test1\", \"test2\", \"test3\")) |>\n  bind_rows(augment(fit) |> \n              calc_miscla_rate(unexpect, .fitted) |> \n              calc_independence() |> \n              calc_metrics(metrics = c(\"harmonic\", \"arithmetic\")) |> \n              mutate(tests = \"test1 OR test3\") |> \n              dplyr::select(-overlapping)) |> \n  bind_rows(regtree_df |> \n              dplyr::select(-overlapping) |> \n              mutate(tests = \"regression tree\")) |>\n  kable(digits = 3, \n        caption = \"Accuracy (precision and recall) and parsimony (independence) metrics for each individual unit test and for the combined test rule (test1 OR test3) derived from the logic regression model. The harmonic and arithmetic means of the three metrics are included to evaluate the quality of the unit tests in  diagnosing unexpected step counts (more than five days with fewer than 8,000 steps).\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Accuracy (precision and recall) and parsimony (independence) metrics for each individual unit test and for the combined test rule (test1 OR test3) derived from the logic regression model. The harmonic and arithmetic means of the three metrics are included to evaluate the quality of the unit tests in  diagnosing unexpected step counts (more than five days with fewer than 8,000 steps).\n\n|tests           | precision| recall| independence| harmonic| arithmetic|\n|:---------------|---------:|------:|------------:|--------:|----------:|\n|test1           |     0.482|  0.964|        1.000|    0.730|      0.815|\n|test2           |     0.214|  1.000|        1.000|    0.450|      0.738|\n|test3           |     0.589|  0.805|        1.000|    0.762|      0.798|\n|test1 OR test3  |     0.821|  0.836|        0.999|    0.879|      0.886|\n|regression tree |     0.821|  0.836|        1.000|    0.879|      0.886|\n\n\n:::\n:::\n\n\n\n# Applications {#sec-examples}\n\nA regression example is produced to illustrate the test selection process for analyzing the effect of PM10 on mortality. The example demonstrates how the test process can be used to select cutoff values in the unit tests and how the test can be used to diagnose an unexpected PM10 coefficient from the generalized linear model. \n\n## Effect of PM10 on mortality {#sec-pm10-mortality}\n\nConsider a generalized linear model (GLM) to study the effect of PM10 on mortality [TODO: provide more context of the mortality-PM10 study]. Analysts may expect a PM10 coefficient between [0, 0.005] after considering the temperature confounding [TODO: reference]. This expectation can be framed into a check that fails, labelled as 1, if the PM10 coefficient is outside the range [0, 0.005]. Multiple factors can impact the PM10 coefficient, such as the sample size, the strength of the correlation between mortality and PM10, and the strength of the correlation between mortality and temperature. Analysts may expect a reasonable sample size to ensure the reliability of the coefficient estimate. Outliers in the three variables can also leverage the coefficient. While these are possible factors that could affect the analysis result, it is not clear the cutoff values for these checks to determine a failure. Here a list of checks are created in @tbl-checks. \n\n| the check fails if ... | \n|--------------| \n| Sample size less than or equal to 200 |\n| Sample size less than or equal to 400 |\n| Sample size less than or equal to 600 |\n| Sample size less than or equal to 800 |\n| Mortality-PM10 correlation less than -0.03 |\n| Mortality-PM10 correlation less than -0.04 |\n| Mortality-PM10 correlation less than -0.05 |\n| Mortality-PM10 correlation less than -0.06 |\n| Mortality-temperature correlation greater than -0.3 | \n| Mortality-temperature correlation greater than -0.35 | \n| Mortality-temperature correlation greater than -0.4 | \n| Mortality-temperature correlation greater than -0.45 | \n| Outlier(s) are presented in the variable PM10 | \n| Outlier(s) are presented in the variable mortality | \n\n: A list of checks considered for the generalized linear model of mortality on PM10 and temperature. The checks are based on the sample size, correlation between mortality and PM10, correlation between mortality and temperature, and univariate outlier detection. Multiple cutoff values are specified for each check to determine a failure. {#tbl-checks} \n\nTo generate replicate of the data, we first simulate the correlation matrix of the three variables in a grid and then use a Gaussian copula to generate a multivariate normal distribution based on the specified correlation matrix and sample size. The multivariate normal distribution is transformed using the normal CDF before the inverse CDF of the assumed distributions of the three variables is applied. To determine the appropriate distribution of each variable, various distributions are fitted and compared. This includes poisson and negative binomial for mortality; gamma, log-normal, exponential, weibull, and normal for pm10 and temperature; and beta for pm10 after rescaling the data to $[0-1]$. AIC is used to determine the best distribution fit for each variable with qq plot presented in @fig-dist-fit to evaluate the fit. AIC suggests a negative binomial distributio nfor mortality, a beta distribution for PM10 (multiple by 100 to obtain the original scale), and a Weibull distribution for temperature. To include the effect of outlier, we add a single outlier to the data for mortality and PM10 [more details].\n\n\n\n::: {#cell-fig-dist-fit .cell}\n\n```{.r .cell-code .hidden}\naa2 <- fitdistrplus::fitdist(pm10$mortality, \"nbinom\")\np1 <- tibble(fitted= rnbinom(558, size = aa2$estimate[1], mu = aa2$estimate[2]),\n       observed= pm10$mortality) %>%\n  ggplot(aes(sample = fitted)) +\n  stat_qq() +\n  stat_qq_line() + \n  theme_bw() + \n  theme(aspect.ratio = 1) + \n  ggtitle(\"mortality: \\nNB(size = 74, mu = 183)\")\n\n\naa6 <- fitdistrplus::fitdist(pm10$pm10[pm10$pm10 < 90]/100, \"beta\")\np2 <- tibble(fitted = rbeta(557, shape1 = aa6$estimate[1], shape2 = aa6$estimate[2]) * 100,\n       observed = pm10$pm10[pm10$pm10 < 90]) %>%\n  ggplot(aes(sample = fitted)) +\n  stat_qq() +\n  stat_qq_line() + \n  theme_bw() + \n  theme(aspect.ratio = 1) + \n  ggtitle(\"pm10: \\n100 * beta(shp1 = 4.21, shp2 = 11.67)\")\n  \n\naa4 <- fitdistrplus::fitdist(pm10$temp, \"weibull\")\np3 <- tibble(fitted= rweibull(558, aa4$estimate[1], aa4$estimate[2]),\n       observed= pm10$temp) %>%\n  ggplot(aes(sample = fitted)) +\n  stat_qq() +\n  stat_qq_line() + \n  theme_bw() + \n  theme(aspect.ratio = 1) + \n  ggtitle(\"temperature: \\nweibull(shape = 3.8, scale = 61)\")\n\np1 | p2 | p3\n```\n\n::: {.cell-output-display}\n![QQ-plot of the distribution fit for mortality, PM10, and temperature based on the fitted distribution from the original data. The fitted distribution is compared to the observed data to assess the distribution fit.](index_files/figure-pdf/fig-dist-fit-1.pdf){#fig-dist-fit fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# cor(pm10$pm10, pm10$mortality)\n# cor(pm10$temp, pm10$mortality)\n# cor(pm10$temp, pm10$pm10)\ncorr_grid <- expand.grid(seq(0.01, 0.2, by = 0.02),                         \n                         seq(0.25, 0.5, by = 0.05),                           \n                         seq(0.25, 0.45, by = 0.05))  \n\ngen_corr_mtx <- function(r1, r2, r3) {   \n  # correlation between mortality and pm10 are negative - r1\n  # correlation between mortality and temp are negative - r2\n  cor_matrix <- matrix(c(1, -r1, -r2,                          \n                         -r1, 1, r3,                          \n                         -r2, r3, 1), nrow = 3, byrow = TRUE)          \n  if (all(eigen(cor_matrix)$values > 0)) return(cor_matrix) \n  }  \n\ncorr_mtx <- lapply(1:nrow(corr_grid), function(i) {   \n  gen_corr_mtx(corr_grid[i, 1], corr_grid[i, 2], corr_grid[i, 3]) }) \ncorr_mtx <- corr_mtx[map_lgl(corr_mtx, ~!is.null(.x))]  \nsample_size <- c(200, 400, 600, 800, 1000)\noutlier <- c(TRUE, FALSE)  \n\ngenerate_data <- function(n, mtx, seed = 123, outlier = FALSE) {   \n  mu <- c(0, 0, 0)   \n  set.seed(seed)  \n  data <- mvrnorm(n, mu, mtx, empirical = TRUE)   \n  U <- pnorm(data, mean = 0, sd = 1)   \n  \n  if (!outlier) {\n   tibble(mortality = qnbinom(U[,1], size = 74, mu = 183),           \n         pm10 = qbeta(U[,2], shape1 = 4.21, shape2 = 11.67) * 100,           \n         temp = qweibull(U[,3], shape = 3.8, scale = 61))    \n  } else{\n    pm10_vec <- qbeta(U[,2], shape1 = 4.21, shape2 = 11.67) * 100\n    tibble(\n      mortality = c(qnbinom(U[,1], size = 74, mu = 183)[-1],\n                    rnorm(n = 1, mean = 300, sd = 10)),           \n      pm10 = c(pm10_vec[-1], rnorm(n = 1, mean = 100, sd = 10)),           \n      temp = qweibull(U[,3], shape = 3.8, scale = 61)\n      )    \n  }\n  \n}\n\nres <- tibble(corr_mtx = corr_mtx) |> \n  mutate(id = row_number()) |> \n  crossing(sample_size, outlier) |> \n  rowwise() |>   \n  mutate(data = list(generate_data(n = sample_size, mtx = corr_mtx, outlier = outlier)),          \n         fit = list(summary(glm(mortality ~ pm10 + temp, family = \"poisson\", data))$coefficients))  \n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: There were 1500 warnings in `mutate()`.\nThe first warning was:\ni In argument: `fit = list(...)`.\ni In row 2.\nCaused by warning in `dpois()`:\n! non-integer x = 310.740123\ni Run `dplyr::last_dplyr_warnings()` to see the 1499 remaining warnings.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\npm10_df <- res |>   \n  mutate(     \n    p_value = fit[2,4],     \n    coef = fit[2,1],     \n    mor_pm10_correlation = corr_mtx[1, 2],     \n    mor_temp_correlation = corr_mtx[1, 3],     \n    temp_pm10_yz_correlation = corr_mtx[2, 3],     \n    unexpect = ifelse(between(coef, 0, 0.005), 0, 1),\n    `smpl_size <= 200` = ifelse(sample_size <= 200, 1, 0),\n    `smpl_size <= 400` = ifelse(sample_size <= 400, 1, 0),\n    `smpl_size <= 600` = ifelse(sample_size <= 600, 1, 0),\n    `smpl_size <= 800` = ifelse(sample_size <= 800, 1, 0),\n    `cor(m, PM10) < -0.03` = ifelse(mor_pm10_correlation < -0.03, 1, 0),\n    `cor(m, PM10) < -0.04` = ifelse(mor_pm10_correlation < -0.04, 1, 0),\n    `cor(m, PM10) < -0.05` = ifelse(mor_pm10_correlation < -0.05, 1, 0),\n    `cor(m, PM10) < -0.06` = ifelse(mor_pm10_correlation < -0.06, 1, 0),\n    `cor(m, tmp) > -0.3` = ifelse(mor_temp_correlation > -0.3, 1, 0),\n    `cor(m, tmp) > -0.35` = ifelse(mor_temp_correlation > -0.35, 1, 0),\n    `cor(m, tmp) > -0.4` = ifelse(mor_temp_correlation > -0.4, 1, 0),\n    `cor(m, tmp) > -0.45` = ifelse(mor_temp_correlation > -0.45, 1, 0)\n  ) |>    \n  ungroup() \n\npm10_df <- pm10_df |> \n  rowwise() |> \n  mutate(`PM10 outlier` = ifelse(any(scale(data$pm10) > 4), 1, 0),\n         `mortality_outlier` = ifelse(any(scale(data$mortality) > 4), 1, 0)\n         ) |> \n  ungroup()\n```\n:::\n\n\n\n::: {#fig-linear-reg-tree}\n\n\n\n::: {.cell fig.heigh='3'}\n\n```{.r .cell-code .hidden}\nfit <- pm10_df |> fit_logic_reg(unexpect, `smpl_size <= 200`:`mortality_outlier`, \n                                seed = 6, nleaves = 4)\nplot_logicreg(fit) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/unnamed-chunk-8-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n<!-- The caption with some LaTeX like this $\\lambda$. -->\n\nLogic regression model fitted to the twelve unit tests and the outcome expectation (unexpect) as the response variable. The model suggests the relationship:\n($\\text{cor}(\\text{m}, \\text{tmp}) > -0.35$ AND $\\text{smpl\\_size} \\le 600$) OR $\\text{cor}(\\text{m},  \\text{PM10}) < - 0.05$\n:::\n\n\n\n::: {#tbl-linear-reg .cell}\n\n```{.r .cell-code .hidden}\nlibrary(kableExtra)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'kableExtra'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ndf <- list(\n  tibble(.fitted = pm10_df$`smpl_size <= 200`, \n         unexpect = pm10_df$unexpect),\n  tibble(.fitted = pm10_df$`cor(m, PM10) < -0.03`, \n         unexpect = pm10_df$unexpect),\n  tibble(.fitted = pm10_df$`cor(m, tmp) > -0.35`, \n         unexpect = pm10_df$unexpect),\n  tibble(.fitted = pm10_df$mortality_outlier, \n         unexpect = pm10_df$unexpect),\n  augment(fit)) |>\n  map_dfr(~.x |> calc_miscla_rate(unexpect, .fitted) |> \n            mutate(overlapping = 0, independence = 1) |> \n            calc_metrics(metrics = c(\"harmonic\", \"arithmetic\")), .id = \"id\") \n\ntbl <- tibble(tests = c(1,2,3,4, 5)) |> bind_cols(df |> dplyr::select(-id)) \ntbl |> \n  kable(digits = 3, escape = FALSE,\n        caption = \"Accuracy (precision and recall) and parsimony (independence) metrics derived from the logic regression model, along with harmonic and arithmetic means, for individual unit tests (1: sample size, 2: mortality-PM10 correlation, 3: mortality-temperature correlation), and the combined test rule 4: (sample size AND mortality-temperature correlation) OR mortality-PM10 correlation.\") \n```\n\n::: {.cell-output-display}\n\n\nTable: Accuracy (precision and recall) and parsimony (independence) metrics derived from the logic regression model, along with harmonic and arithmetic means, for individual unit tests (1: sample size, 2: mortality-PM10 correlation, 3: mortality-temperature correlation), and the combined test rule 4: (sample size AND mortality-temperature correlation) OR mortality-PM10 correlation.\n\n| tests| precision| recall| overlapping| independence| harmonic| arithmetic|\n|-----:|---------:|------:|-----------:|------------:|--------:|----------:|\n|     1|     0.087|  0.215|           0|            1|    0.175|      0.434|\n|     2|     0.988|  0.610|           0|            1|    0.822|      0.866|\n|     3|     0.392|  0.581|           0|            1|    0.569|      0.658|\n|     4|     0.649|  0.641|           0|            1|    0.732|      0.763|\n|     5|     0.760|  0.880|           0|            1|    0.869|      0.880|\n\n\n:::\n:::\n\n\n\nA logic regression is fitted to predict whether the PM10 coefficient is unexpected (outside the range of [0, 0.005]) using the checks listed in @tbl-checks. Precision, recall, and independence score, along with their harmonic and arithmetic mean are calculated. @fig-linear-reg-tree shows the logic regression tree from the fitted model and @tbl-linear-reg prints the numerical summary of four selected single test and their combined test found by the logic regression. The logic regression model picks up the following cutoff value for each type of check:\n\n-   sample size *larger than* 200\n-   mortality-temperature correlation greater than -0.35\n-   mortality-PM10 correlation less than -0.04\n-   mortality contain outliers that are detected by the univariate outlier detection\n\n<!-- (mortality-temperature correlation > -0.35 OR there exist mortality outlier) and (mortality-PM10 correlation < -0.04 AND (not sample size less than 200) )  -->\n\nThe tree structure suggests checking mortality-PM10 correlation and a sample size larger than 200 with an additional check of either outlier on mortality or correlation between mortality and temperature. This combined check rule generates a 0.76 precision and a 0.88 recall for predicting the unexpected PM10 coefficient. The single check, `cor(m, PM10) < -0.03`, is also powerful with a high precision of 0.988, but the low recall value of 0.61 suggests its high false positive rate, as compared to the combined rule suggested by the logic regression.\n\n# Discussion {#sec-discussion}\n\nTODO\n\n-   how to systematically simulate data is still unknown\n\n-   plotting is a critical way to check data and they can still be frame into checks. i.e. the density/ histogram suggests there are outliers. It is a open problem to how to encode the visualization into checks. \n\n-   currently no automated way to generate checks. It is interesting to see how check generation can be automated, although it requires the inputs from experts across a wide array of common scenarios.\n\n- checks are also closely related to the concept of unit tests in software engineering. While unit tests are designed to isolate and test specific aspects of the code, it is difficult for analysis validation check to do so.\n\n-   There are cost and benefit on setting expectation on different granularity. At the lowest level, one may have a plan for each data entry and every data handling steps. This requires more work from the analysts and may not be practical in practice. For more complex analyses, analysts may divide the analysis into sections and set expectations for each. They can then focus on the specific sections flagged by the tests and sub-divide the sections to set expectation and diagnose the analysis in a hierarchical manner.\n\n\n# Conclusion {#sec-conclusion}\n\nTODO\n\n# Acknowledgement\n\nThe article is created using Quarto [@Allaire_Quarto_2022] in R [@R].\nThe source code for reproducing the work reported in this paper can be\nfound at: <https://github.com/huizezhang-sherry/paper-avc>. \n\n# References\n",
    "supporting": [
      "index_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "\\usepackage{booktabs}\n\\usepackage{longtable}\n\\usepackage{array}\n\\usepackage{multirow}\n\\usepackage{wrapfig}\n\\usepackage{float}\n\\usepackage{colortbl}\n\\usepackage{pdflscape}\n\\usepackage{tabu}\n\\usepackage{threeparttable}\n\\usepackage{threeparttablex}\n\\usepackage[normalem]{ulem}\n\\usepackage{makecell}\n\\usepackage{xcolor}\n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}