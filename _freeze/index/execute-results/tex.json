{
  "hash": "bd02edc9ac8b34dc6138dcb1a8f0df07",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"My wonderful paper\"\nabstract: \"Unexpected results in data analysis can prompt researchers to examine data quality and analysis steps. While some researchers can typically diagnose issues effectively with a few checks, others may struggle to identify appropriate checks to diagnose the problem. [an example of check] These checks are often informal and difficult to trace and discuss in publications, resulting in others questioning the trustworthiness of the analysis. To address this, we formalize the informal checks into an *analysis plan* that encompasses the analysis steps and (the unit tests): one test for whether the result meets expectations and multiple tests for checking the analysis. We then present a procedure to assess the quality of these unit tests based on their accuracy and redundancy on simulated versions of the original data. The accuracy is assessed using binary classification metrics, *i.e.*, precision and recall, while redundancy is calculated using mutual information. This procedure can be used to conduct a sensitivity analysis, compare different analysis plans, and to identify the optimal cutoff point for the unit tests.\"\ncite-method: natbib\nbibliography: references.bib\ndocumentclass: jds\nformat:\n  pdf:\n   keep-tex: true\n   toc: true\npreamble: >\n  \\usepackage{amsfonts,amsmath,amssymb,amsthm}\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\nknitr::opts_chunk$set(echo = FALSE)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.1\nv ggplot2   3.5.1     v tibble    3.2.1\nv lubridate 1.9.3     v tidyr     1.3.1\nv purrr     1.0.2     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(patchwork)\nlibrary(MASS)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(ggh4x)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'ggh4x'\n\nThe following object is masked from 'package:ggplot2':\n\n    guide_axis_logticks\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\npm10 <- read_csv(here::here(\"data\", \"pm10.csv\")) |> \n  filter(!is.na(pm10)) |>    \n  mutate(season = as.factor(season))  \n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRows: 3288 Columns: 6\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (1): season\ndbl  (4): mortality, pm10, temp, dewpt\ndate (1): date\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n\n\\newpage\n\n# Introduction\n\nIn a data analysis, an experienced researcher can often quickly assess whether a result meets their expectation, while others may be unsure of what to anticipate from the analysis or how to recognize when results diverge form expected norms. (These expectations are often based on prior knowledge, domain expertise, or common sense and they can often be framed into a Boolean question that can be answered by a unit test. For example, in a linear regression model, one may expect the p-value of the coefficient to be less than 0.05. On top of the expectation on the final results, researchers may also have expectations on the intermediate steps of the analysis process. For example, one may expect the data doesn't contain outliers, etc. These expectations are often checked early on to prevent an unexpected propagate through the analysis process.)\n\nThis expectation-setting process is often implicit and rarely documented or discussed in publications. (There are most publication requires open source code for publication, however, it is no instruction or guidance on checking ...).  Yet, making them explicit is crucial for 1) helping junior researchers interpret results and diagnose the unexpected, 2) providing checkpoints that support independent replication or application of methods to new data, 3) aligning assumptino across researchers from different backgrounds.\n\nIn this paper, we formalize these informal checks into an *analysis plan* comprising the analysis steps along with the associated unit tests. This formalization allows us to examine assumptions made about the data during analysis and to compare different unit tests in diagnosing unexpected outcomes. We then introduce a procedure to assess the quality of these unit tests based on their accuracy and redundancy across simulated version of the original data. The accuracy is assessed using binary classification metrics -- precision and recall -- while redundancy is measured using mutual information. We illustrate these concepts with a simple step count example and a scenario of diagnosing unexpected regression results when studying the effect of PM10 on mortality.\n\nThe rest of the paper is organized as follows: @sec-plan describes the concept of analysis plan in detail. @sec-examples provides examples of analysis plan [more details]. (need another section here or before examples?) @sec-conclusion concludes the paper.\n\n# Literature review\n\n## Diagnosing unexpected outcomes in data analysis\n\nTODO: what if the expectation is \"wrong\"\n\n## Unit tests\n\n## Data quality checks\n\n- `pointblank`, `assertthat`\n\n# Analysis plan {#sec-plan}\n\n## Framing checks into unit tests\n\nQ: Whether we should formulate these concept with math notation?? A: only if it helps\n\n<!-- The analysis plan described in this paper should be differentiated from the pre-specifies analysis plan document often used in biostatistics to specifies the hypothesis, data collection mechanism, statistical procedures etc of randomized experiments.  -->\n\nAn analysis plan is a set of analysis steps combined with expectations. Expectations represent our belief about certain aspects of the analysis, independent of the analysis itself. It can be divided into two types: *outcome expectation* and *plan expectation*. Outcome expectation refers to what we anticipate from the main result of the analysis based on prior knowledge. They shape how we interpret the results and assess whether they are consistent with existing knowledge or indicate the need for updates [@grolemund_cognitive_2014]. For example, in public health, prior research shows the average increase in mortality rate per unit increase in PM10 is about 0.50% [@liu2019ambient]. This serves as an expectation for similar future studies. Plan expectations concern the intermediate steps within the analysis rather than the final outcome. They serve as checkpoints to detect deflection in the analysis process For example, we may expect temporal data to be ordered with no gaps and duplicates, or expect that temperature will be a significant covariate in the linear regression model of PM10 on mortality.\n\n(might be useful) Analysis plans can be constructed at various granularities, at the highest level, one may only has a plan of the specific method used for analysing data and the expected outcome. This provides little guidance when a deviation from expectation occurs. At the lowest level, one may have a plan for each data entry and every data handling steps. This provides too much detail and may not be practical in practice. \n\nExperienced analysts often have implicit expectation about the outcome and rely on a few \"directional signs\" to check when the outcome deviate from those expectation. However, these expectations are rarely made explicit within the analysis workflow. This makes it challenging for consumers of  the analysis to evaluate the results, since it becomes difficult to disentangle whether discrepancies arise from differing expectations or from the use of statistical technique, without running the analysis themselves. Non-expert analysts, lacking prior knowledge or instinct, may not have clear expectations of the results. This can lead to reduced confidence of the analysis and makes it more difficult and time-consuming to diagnose the cause of the deviation when the results don't align with expectations. By explicitly formulating these expectations, an analysis plan can guide the analysis process, facilitate the communication and evaluate the validity of the results.\n\nThe expectations can be thought of as a set of unit tests used to validate the results of data analysis. By specifying a range of values for these tests, multiple versions of the dataset can be generated to satisfy different sets of plan expectations. This allows us to present what we called the \"result universe\" -- the complete set of possible results that can be obtained from one data analysis process. By visualizing the result universe, data analysis consumers can observe how changes in expectations affect the results and the range of alternative outcomes that could arise under different conditions. This enables them to evaluate the outcomes based on their own plan expectations and gain a broader perspective on how the actual results produced by analysts fit within this spectrum of possibilities, promoting transparency and trust in analysis.  \n\nFurthermore, by generating multiple versions of the data, we can emulate various scenarios within the same context for students to exercise judgement when conducting data analysis in a classroom setting. \n\n## A toy example\n\nLet's think about a 5-day step count. You make a resolution to walk on average 5000 steps a day (your expectation) and using an app to record your step count. After 5 days, the app tells you've walked on average 8000 steps.\n\nIt is easy to come up with reasons why an 8000 average step is resulted based on common sense:\n\n1. you may run a 10k on day 1, resulting a high step count on the day (outlier on the right).\n2. you left your phone at home on day 3, resulting a zero or minimal step count on the day (outlier on the left).\n3. you may realise the step count may increase since you were in a hiking trip in the last five days (average shift).\n\nBased on these reasons, you may devise a set of unit tests to check the step count data, i.e. check the maximum and minimum step count, check the difference between each day.\n\n- If the daily count looks like c(4000, 5000, 5500, 5500, 20000), the maximum check will flag the data for investigate the maximum. The difference between days test will also flag the data\n- If the daily count looks like c(20000, 20000, 20000, 20000, 20000), the maximum check will flag the data for investigate the maximum.\n\nSome part of the space is impossible: c(0, 4000, 5000, 5500, 5500) is flagged by the minimal tests but won't cause an average of 8000 average step.\n\nThe statistical procedure of averaging 5 numbers \"around 5000\" to get a mean of 5000 is *consistent* meaning if all the numbers are around 5000, we are guaranteed to get a mean around 5000. We could devise 5 unit tests to check each number. Since you're more familiar with your daily life, you may realise the step count may increase since you were in a hiking trip in the last two days. This may prompt you to check the step count.\n\nIn a data analysis, it is not practical to check every entry of the data, a similar strategy of devising tests to check for\n\n- The combination of unit tests are not unique\n- The unit tests provide guidance for diagnosing the results, but are not red flags: c(2000, 2000, 5000, 8000, 8000) will likely to fail the max diff test but receive a within expectation mean.\n\n# Method \n\n## A workflow to assess the quality of unit tests\n\n- multiple tests can be generated to diagnose different aspects of an analysis \n- good tests \n    1) are responsive to an unexpected result - accuracy, \n    2) motivate actions of analysts to investigate the results. This points towards a smaller set of independent tests\n\n- Since both the plan/ outcome expectations can be phrased as unit tests, which output binary outcomes, consider using the plan expectations as predictors of the outcome expectations. This allows to generate the confusion matrix of predicting the outcome with the plan expectation. \n\n- For an analysis, ideally, good plan expectations should maximize the detection of unexpected outcomes while minimize the false positive discovery, which suggest the use of precision and recall (REF) for evaluating the performance of the plan expectations.\n\n  * precision: the proportion of unexpected results (TP) out of all the predicted unexpected results (TP + FP)\n  * recall: the proportion of unexpected results (TP) out of all the actual unexpected results (TP + FN)\n\n* A logic regression (ref) is used to model the relationship between the plan and outcome expectations. (justify the use of Logic regression)\n\n* on independence of the tests\n\n\n## Toy example revisited \n\n* provide interpretation at different scenarios:\n    \n  1) one test is flagged, the prediction is as expected: \n  2) multiple tests are flagged, the prediction is unexpected, \n  3) no test is flagged, the prediction is unexpected, \n  4) no test is flagged, the prediction is as expected\n\n\n# Applications {#sec-examples}\n\nThree examples are presented to illustrate how the concept of analysis plan can be applied to data analysis. [toy example]. @sec-linear-reg illustrates how constructing  the result universe in a linear regression model of PM10 on mortality can help understand the impact of sample size, model specification, and variable correlation structure on data analysis. [example three]\n\n\n## Linear regression {#sec-linear-reg}\n\nConsider a linear regression model to study the effect of PM10 on mortality (provide context of using PM10 to study mortality). Analysts may expect a significant (p-value $\\le$ 0.05) PM10 coefficient in the linear model from the literature. This is the *outcome expectation*. There are multiple factors that can affect the outcome expectation of linear regression, which here is called *plan expectation*, for example, 1) sample size, 2) model specification, and 3) correlation structure between variables. Adequate sample size is required to achieve the desired power to detect the significance of PM10 on mortality. Temperature is often an important confounder to consider in such study (add reference). From some domain knowledge, an analyst may expect that the significance of PM10 coefficient can be attained by adding temperature to the model. Analysts may also expect certain correlation structure between PM10, temperature, and mortality, and the distribution of each variable. \n\nTo build the result universe, datasets can be simulated to either meet and fail these plan expectations, allowing the analysts to observe the significance of PM10 coefficient. Here, sample sizes of 50, 100, 500, and 1000 are considered. Two model specifications are included: 1) linear model with PM10 as the only covariate ($\\text{mortality} \\sim \\text{PM10}$), 2) linear model with PM10 and temperature as covariates ($\\text{mortality} \\sim \\text{PM10} + \\text{temp}$). A grid-based approach is used to simulate correlation structure. Reasonable ranges of correlation between the three variables are $\\text{cor}(\\text{mortality}, \\text{PM10}) \\in [-0.01, 0]$, $\\text{cor}(\\text{mortality}, \\text{temperature}) \\in [-0.6, -0.2]$, and $\\text{cor}(\\text{PM10}, \\text{temperature}) \\in [0.2, 0.6]$. \n\n * add a paragraph to describe the simulation process\n\n * add a fourth panel to describe the comparison of a right/ wrong expectation, i.e. correlation on PM10 and mortality\n\n@fig-result-universe shows that result universe of the linear regression model and how a change of decision in one of the plan expectations above affect the outcome expectation. Panel a) is colored by the outcome expectation -- whether a significant p-value is found in the PM10 coefficient. Panel b) shows the effect of adding temperature to the model and the results show that the significance of PM10 coefficient can be achieved by adding temperature to the model for a sample size of 500. Panel c) shows that increasing sample size from 50 to 100 enhances the significance of p-value for PM10 and the significance remains with further increases in sample size. [note: weave the \"actual data\" into the example linear regression model]\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Generate correlation matrices \ncorr_grid <- expand.grid(seq(-0.01, -0.001, by = 0.001), \n                         seq(-0.6, -0.2, 0.05), seq(0.2, 0.6, 0.05))\n\n# Function to compute correlation matrix for each combination \ngen_corr_mtx <- function(r1, r2, r3) {   \n  cor_matrix <- matrix(c(1, r1, r2,                          \n                         r1, 1, r3,                          \n                         r2, r3, 1), nrow = 3, byrow = TRUE)          \n  # Store the matrix in the list   \n  if (all(eigen(cor_matrix)$values > 0)) return(cor_matrix) \n  }  \n\n# Plan for parallel processing\ncorr_mtx <- lapply(1:nrow(corr_grid), function(i) {   \n  gen_corr_mtx(corr_grid[i, 1], corr_grid[i, 2], corr_grid[i, 3]) \n  }) \ncorr_mtx <- corr_mtx[map_lgl(corr_mtx, ~!is.null(.x))]  \nsample_size <- c(50, 100, 500, 1000) \nmodel <- c(\"mortality ~ pm10 + temp\", \"mortality ~ pm10\") |> map(as.formula) \n\ngenerate_data <- function(n, mtx, seed = 123) {   \n  mu <- c(0, 0, 0)   \n  data <- mvrnorm(n, mu, mtx, empirical = TRUE)   \n  U <- pnorm(data, mean = 0, sd = 1)   \n  set.seed(seed)      \n  tibble(mortality = qpois(U[,1], 182), # assume distribution\n         pm10 = qgamma(U[,2], shape = 6, rate = 0.2),           \n         temp = qnorm(U[,3], mean = 55, sd = 16))   \n  }  \n\n# test for dist fit\n# fitdist(pm10$pm10, \"gamma\") -> pm10_fit\n# plot(pm10_fit) # gamma is better than normal for the fit of pm10\n#fitdist(pm10$temp, \"norm\") -> temp_fit\n# plot(temp_fit)\n\nres <- tibble(corr_mtx = corr_mtx) |> \n  mutate(id = row_number()) |> \n  crossing(sample_size, model) |> \n  rowwise() |>   \n  mutate(data = list(generate_data(n = sample_size, mtx = corr_mtx)),          \n         fit = list(summary(lm(model, data))$coefficients))  \n```\n:::\n\n::: {#cell-fig-result-universe .cell}\n\n```{.r .cell-code .hidden}\ndt <- res |>   \n  mutate(     \n    p_value = fit[2,4],     \n    coef = fit[2,1],     \n    xy_correlation = as.numeric(corr_mtx[1, 2]),     \n    xz_correlation = as.numeric(corr_mtx[1, 3]),     \n    yz_correlation = as.numeric(corr_mtx[2, 3]),     \n    fml = deparse(model) |> as.factor(),     \n    expect = ifelse(p_value < 0.05, 1, 0) |> as.factor(),     \n    ) |>    \n  ungroup() \n\ncode_tbl <- crossing(V1 = c(\"signifpm10\", NA),           \n                     V2 = c(\"smallsample\", NA), \n                     V3 = c(\"withtemp\", NA)) |>      \n  mutate(col1 = paste0(V1,\"_\", V2, \"_\", V3))   \n\nlookup_tbl <- tibble(   \n  value = crossing(x = c(1, 0), y = c(1, 0), z = c(1, 0)) |>      \n    mutate(col = paste0(x,\"_\", y, \"_\", z)) |> \n    arrange(-x, -y, -z) |>      \n    pull(col),   \n  plan1 = unique(code_tbl$col1) )  \n\ndt2 <- dt |>    \n  mutate(T1 = ifelse(sample_size < 200, 1, 0),          \n         T2 = ifelse(fml == \"mortality ~ pm10 + temp\", 1, 0),          \n         pp1 = paste0(expect, \"_\", T1, \"_\", T2),) |>    \n  left_join(lookup_tbl, by = c(\"pp1\" = \"value\"))      \n\np0 <- dt |> ggplot() +   \n  geom_point(aes(x = p_value , y = coef), color = \"grey\", size = 0.5) +  \n  labs(x = \"p-value\", y = \"coefficient\") +   \n  theme_bw() +    \n  theme(aspect.ratio = 1, \n        panel.grid.minor = element_blank()) \n\ndf0 <-  dt |> filter(sample_size == 500, xy_correlation == -0.005, \n                     between(xz_correlation, -0.46, -0.43),\n                     yz_correlation == 0.4) |> arrange(expect)\ndf2 <- df0 |> filter(fml == \"mortality ~ pm10 + temp\") # test for model spec\n\np1 <- p0 + \n  geom_point(aes(x = p_value , y = coef, color = expect), size = 0.3) +  \n  scale_color_brewer(palette = \"Dark2\", direction = -1) +\n  labs(title = \"Coefficient vs. p-value\") + \n  theme(legend.position = 'none')\n\np2 <- p0 + \n  geom_point(data = df0, aes(x = p_value, y = coef, color = expect), size = 2) +\n  geom_path(data = df0, aes(x = p_value, y = coef), color = \"black\",\n            arrow = arrow(type = \"open\", angle = 30, \n                          length = unit(0.1, \"inches\"))) +\n  ggrepel::geom_label_repel(\n    data = df0, aes(x = p_value, y = coef, label = fml, color = expect)\n    ) +\n  scale_color_brewer(palette = \"Dark2\", direction = -1) +\n  labs(title = \"Effect of adding temperature\") \n\ndf00 <-  dt |> filter(xy_correlation == -0.005, \n                     between(xz_correlation, -0.46, -0.43),\n                     yz_correlation == 0.4) |> arrange(expect)\ndf <- df00 |> filter((fml == \"mortality ~ pm10 + temp\"))\n\n\np3 <- p0 + \n  geom_point(data = df, aes(x = p_value, y = coef, color = expect), size = 2) +\n  geom_path(data = df, aes(x = p_value, y = coef), color = \"black\",\n            arrow = arrow(type = \"open\", angle = 30, \n                          length = unit(0.1, \"inches\"))) +\n  ggrepel::geom_label_repel(\n    data = df, aes(x = p_value, y = coef, label = sample_size, color = expect)\n    ) +\n  scale_color_brewer(palette = \"Dark2\", direction = -1) +\n  labs(title = \"Effect of sample size\") \n\n(p1 | p2) / (p3 | plot_spacer()) + \n  plot_annotation(tag_levels = \"a\") + \n  plot_layout(guides = 'collect') &\n  theme(legend.position = 'bottom') \n```\n\n::: {.cell-output-display}\n![The result universe of linear regression model to study the effect of PM10 on mortality: a) colored by whether the p-value of PM10is significant (less than 0.05), b) the effect of adding temperature to the model for a sample size of 500, c) the effect of increasing sample size for a fixed correlation structure.](index_files/figure-pdf/fig-result-universe-1.pdf){#fig-result-universe fig-pos='H'}\n:::\n:::\n\n\n\n# Discussion\n\n* how to systematically simulate data is still unknown, sensitivity of the simulation to the results \n\n* plotting is a critical way to check data and they can still be frame into a unit test. it is a open problem to how to encode the visualization into the unit tests. Maybe a procedure like confirm plot (this looks alright to you) and then press the button to continue\n\n* currently no automated way to generate unit tests\n\n\n# Conclusion {#sec-conclusion}\n\n\n# References\n",
    "supporting": [
      "index_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}